{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The Notebook","text":"<p>Welcome to my notes! \ud83d\udc4b</p>"},{"location":"#about-this-documentation","title":"About this documentation","text":"<p>This notebook is a collection of ideas, code snippets, projects, interesting articles, links to blog posts, and anything else catching my attention. Basically it's my \ud83c\udf31 digital garden.</p> <p>\ud83d\udcd6 \"By relying on an internal database (or \u201csecond brain\u201d) they can reduce their reliance on external search engines. I've found when encountering a similar issue it is often more efficient to reference my own notes as I\u2019ve already written them in a way that makes sense to me rather than to go through the process of parsing through search results until I encounter what I am looking for and then go through the process of translating how it\u2019s written into a mental model that works for me.\" -- M.Powell, The ReadME Project.</p>"},{"location":"#workflow-behind-these-docs","title":"Workflow behind these docs","text":"<p>This notebook is a static website based on markdown documentation built with Material for MkDocs.</p> <p>The website configuration (including navigation, custom themes, extensions, plugins, etc.) is available in the mkdocs.yml file.</p> <p>Publishing is made on GitHub Pages and is handled by GitHub Actions with a cloud-hosted runner. </p> <p>The workflow YAML file is available here and is triggered by any push on the <code>main</code> branch.</p>"},{"location":"#similar-brilliant-projects","title":"Similar brilliant projects","text":"<ul> <li> javierobcn/Notas</li> </ul>"},{"location":"projects/","title":"Projects","text":"<p>\"The better way to get a project done faster is to start sooner\"- Jim Highsmith</p> <p></p>"},{"location":"projects/#working-on","title":"Working on...","text":"<ul> <li> CRM dockerized web application: Django, Postgresql, Bootstrap, Docker</li> <li> Software support service ticket manager: Django, SQLite3, Bootstrap, Docker</li> </ul>"},{"location":"projects/#just-ideas-for-now","title":"Just ideas for now","text":"<ul> <li><code>etf-tracker</code>, an ASP.NET Core web application to track ETFs from multiple trading platforms</li> <li><code>oci-shape-monitor</code>, a web client to track the availability of different OCI shapes (inspired by  this idea made in PHP)</li> <li><code>lang-progress-tracker</code>, a WebAPI client interacting with the GitHub API to track the progress of the languages used in GitHub, based on the number of commits </li> <li>Convert <code>porelli/bash_scripts/Move_containers.sh</code> in PowerShell and test on Windows Containers</li> </ul>"},{"location":"projects/#maybe-one-day","title":"Maybe one day","text":"<p>General \"build-your-own-x\" projects:</p> <ul> <li>Python: Build a stock market predictor, a Web Server, a Docker mocker, a Continuous Integration system</li> <li>Ruby: Build a collaborative text editor, UDP, git</li> <li>Rust exercise: Build a DNS Server, a browser engine, a OS</li> </ul>"},{"location":"cloud/aws/","title":"aws","text":""},{"location":"cloud/aws/#aws-cli","title":"AWS CLI","text":"<ul> <li>Installation instructions on Linux: <pre><code>$ curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n$ unzip awscliv2.zip\n$ sudo ./aws/install\n</code></pre> For test: <pre><code>$ aws --version\n</code></pre></li> <li>Authenticate the workstation against the AWS API:<ul> <li>Create an Access Key in AWS Management Console &gt; User account &gt; Security Credentials &gt; Access Key</li> <li>On AWS CLI, <code>aws configure</code></li> <li>Insert the generated Access Key ID </li> <li>Insert the corresponding generated Secret Access Key</li> <li>Insert your reference region (using region naming convention)</li> </ul> </li> </ul>"},{"location":"cloud/aws/#ec2","title":"EC2","text":""},{"location":"cloud/aws/#set-up-a-remote-dev-environment-on-an-ec2-debian-machine","title":"Set up a remote dev environment on an EC2 Debian machine","text":"<ul> <li>Test SSH to remote host: <code>ssh -i FULL/PATH/TO/PRIVATE/KEYFILE.pem USER@REMOTE-HOSTNAME</code> <p>-i: A file from which the identity key (private key) for public key authentication is read.</p> </li> <li>Configure WSL extension (the ideal one is the Remote Development) on VSCode (a powerful guide in the VSCode docs)<ul> <li>Guide: https://code.visualstudio.com/docs/remote/ssh#_remembering-hosts-you-connect-to-frequently</li> <li>Reduce permissions on the private key: https://superuser.com/questions/1296024/windows-ssh-permissions-for-private-key-are-too-open</li> </ul> </li> <li>Open and save SSH VSCode workspace</li> <li>Forward used ports (<code>Rails</code> uses port 3000): <code>ssh -L 3000:localhost:3000 USER@REMOTE-HOSTNAME</code></li> <li>Ready to go! </li> </ul>"},{"location":"cloud/aws/#remote-vscode-over-ssh-crashes-ec2-instance","title":"Remote VSCode over SSH crashes EC2 instance","text":"<p>The main symptom is that the instance is not reachable via SSH anymore. Using <code>ssh -v</code>, the connection hangs with something like</p> <pre><code>OpenSSH_7.4p1, LibreSSL 2.5.0\ndebug1: Reading configuration data /Users/UserName/.ssh/config\ndebug1: Reading configuration data /etc/ssh/ssh_config\ndebug1: /etc/ssh/ssh_config line 53: Applying options for *\ndebug1: Connecting to host.domain [123.456.123.456] port 22.\ndebug1: Connection established.\ndebug1: identity file /Users/UserName/.ssh/ke&gt; type 1\ndebug1: key_load_public: No such file or directory\ndebug1: identity file /Users/UserName/.ssh/key-cert type -1\ndebug1: Enabling compatibility mode for protocol 2.0\ndebug1: Local version string SSH-2.0-OpenSSH_7.4\n</code></pre> <p>According to  this GitHub issue, the problem may be related to a corrupted log file. </p> <p>Deleting all <code>~/.vscode-server/*.log</code> and <code>*.pid</code> files should solve the issue - the deletion apparently has not effect on VSCode Server whatsoever.</p> <p>For additional troubleshooting, check <code>~/.vscode-server/data/logs/DATETIME/remoteagent.log</code> where <code>DATETIME</code> is the crashing datetime.</p>"},{"location":"cloud/aws/#s3","title":"S3","text":""},{"location":"cloud/aws/#connect-your-web-project-with-an-s3-bucket","title":"Connect your web project with an S3 bucket","text":"<ul> <li>Create your S3 bucket on aws.amazon.com</li> <li>In Properties &gt; Permissions &gt; CORS (Cross-Origin Resource Sharing) Configuration, add <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;CORSConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"&gt;\n&lt;CORSRule&gt;\n&lt;AllowedOrigin&gt;*&lt;/AllowedOrigin&gt;\n&lt;AllowedMethod&gt;GET&lt;/AllowedMethod&gt;\n&lt;AllowedMethod&gt;POST&lt;/AllowedMethod&gt;\n&lt;AllowedMethod&gt;PUT&lt;/AllowedMethod&gt;\n&lt;AllowHeader&gt;*&lt;/AllowHeader&gt;\n&lt;/CORSRule&gt;\n&lt;/CORSConfiguration&gt;\n</code></pre> Or, in JSON format <pre><code>[\n{\n\"AllowedOrigins\": [\n\"*\"\n],\n\"AllowedMethods\": [\n\"GET\",\n\"POST\",\n\"PUT\"\n],\n\"AllowedHeaders\": [\n\"*\"\n],\n}\n]\n</code></pre></li> <li>Go to <code>IAM &gt; Users</code> and create a new user selecting <code>Programmatic Access Type</code></li> <li>To the same user, in <code>Set Permissions</code>, select <code>Attach existing policies direcly</code> and choose <code>AmazonS3FullAccess</code></li> <li>This will create a special Access key ID and Secret Access Key that you can copy and paste in your secrets management function</li> </ul>"},{"location":"cloud/azure-pipelines/","title":"azure pipelines","text":""},{"location":"cloud/azure-pipelines/#build-and-publish-a-build-task","title":"Build and publish a build task","text":""},{"location":"cloud/azure-pipelines/#requirements","title":"Requirements:","text":"<ul> <li>An organization in Azure DevOps, with a project and a pipeline</li> <li>NodeJS, last release (check with <code>node -v</code>), &gt;= 10.x</li> <li>TypeScript compiler &gt;= 2.2.0</li> <li><code>tfs-cli</code> (Cross-platform CLI for Azure DevOps)<ul> <li><code>npm install -g tfx-cli</code></li> </ul> </li> <li>A <code>home</code> folder for the project, it will contain: <pre><code>/readme.md      # readme\n/images\n  /extension-icon.png   # task's icon\n/BuildAndReleaseTaskName    # Folder containing the task and all its scripts\n  /...\n/vss-extension.json     # task's manifest\n</code></pre></li> </ul>"},{"location":"cloud/azure-pipelines/#scaffold-the-task","title":"Scaffold the task","text":"<ul> <li>Create a home folder for the project (e.g. <code>BuildAndReleaseTaskName</code>)</li> <li>Create the subfolder containing the task itself (you may call it the same as the home folder), and <code>cd</code> into it</li> <li>Install the dependencies:<ul> <li><code>npm init</code>: creates the packages.json file, answer the prompt interactively</li> <li><code>npm install azure-pipelines-task-lib --save</code>: package for Azure Pipelines</li> <li><code>npm install @types/node --save-dev</code>: types definition for nodejs, only dev environment</li> <li><code>npm install @types/q --save-dev</code>: types definition for Q,only dev environment</li> </ul> </li> <li><code>echo node_modules &gt; .gitignore</code> to ignore the <code>node_modules/</code> folder<ul> <li>It doesn't make any sense to bring everywhere all the packets, you can always restore them from the packages.json file</li> <li>It would also increase significantly the repository size</li> </ul> </li> <li>Install and setup TypeScript<ul> <li><code>npm install -g typescript</code>: installing TypeScript, globally</li> <li><code>tsc --init</code>: creating the tsconfig.json file</li> </ul> </li> </ul>"},{"location":"cloud/azure-pipelines/#create-the-task","title":"Create the task","text":"<ul> <li>Create the task.json file</li> <li>Copy the task.json template from here, also copied below in case of dead link: <pre><code>{\n\"$schema\": \"https://raw.githubusercontent.com/Microsoft/azure-pipelines-task-lib/master/tasks.schema.json\",\n\"id\": \"{{taskguid}}\",\n\"name\": \"{{taskname}}\",\n\"friendlyName\": \"{{taskfriendlyname}}\",\n\"description\": \"{{taskdescription}}\",\n\"helpMarkDown\": \"\",\n\"category\": \"Utility\",\n\"author\": \"{{taskauthor}}\",\n\"version\": {\n\"Major\": 0,\n\"Minor\": 1,\n\"Patch\": 0\n},\n\"instanceNameFormat\": \"Echo $(samplestring)\",\n\"inputs\": [\n{\n\"name\": \"samplestring\",\n\"type\": \"string\",\n\"label\": \"Sample String\",\n\"defaultValue\": \"\",\n\"required\": true,\n\"helpMarkDown\": \"A sample string\"\n}\n],\n\"execution\": {\n\"Node10\": {\n\"target\": \"index.js\"\n}\n}\n}\n</code></pre></li> <li>Replace the placeholders with your values:<ul> <li><code>\"id\"</code>: a unique GUID for your task, if needed use guidgen.com, a web version of the GuidGen tool from Microsoft</li> <li><code>\"name\"</code>: name without spaces</li> <li><code>\"friendlyname\"</code>: descriptive name, it may include spaces</li> <li><code>\"description\"</code>: detailed description about what the task is doing</li> <li><code>\"author\"</code>: short string describind the author</li> <li><code>\"instanceNameFormat\"</code>: it describes how the task will be viewed in the tasks list from the build/release pipeline<ul> <li>You may also use a variable name with <code>$(variablename)</code></li> </ul> </li> <li><code>\"groups\"</code>: it describes the items where the logical properties of the task can be grouped</li> <li><code>\"inputs\"</code>: inputs necessary to the task's execution</li> <li><code>\"execution\"</code>: execution options for the task, including scripts </li> </ul> </li> <li>Create the index.ts file and fill it with the template available here, also reference for dead link: <pre><code>import tl = require('azure-pipelines-task-lib/task');\n\nasync function run() {\ntry {\nconst inputString: string | undefined = tl.getInput('samplestring', true);\nif (inputString == 'bad') {\ntl.setResult(tl.TaskResult.Failed, 'Bad input was given');\nreturn;\n}\nconsole.log('Hello', inputString); // It just prints Hello followed by the input string\n}\ncatch (err) {\ntl.setResult(tl.TaskResult.Failed, err.message);\n}\n}\n\nrun();\n</code></pre></li> <li>Compile the TypeScript files into Javascript:<ul> <li><code>tsc</code>: it basically build the current project, i.e. the tsconfig.json in the local folder</li> </ul> </li> <li>Test the task with PowerShell: from the root folder run <code>node index.js</code><ul> <li>Using the template above, it will return the error <code>Input required:  samplestring</code></li> <li>Create the input samplestring with an environment variable<ul> <li><code>$env:INPUT_SAMPLESTRING=whoever_you_are</code></li> </ul> </li> <li>Run the task again: this time it will print <code>\"Hello whoever_you_are\"</code> </li> </ul> </li> </ul>"},{"location":"cloud/azure-pipelines/#package-the-task","title":"Package the task","text":"<ul> <li>In the home folder, create the subfolder <code>images</code> and add the task icon, naming it <code>extension-icon.png</code> (128x128)</li> <li>Create the file vss-extension.json using the template here: <pre><code>{\n\"manifestVersion\": 1,\n\"id\": \"build-release-task\",\n\"name\": \"Fabrikam Build and Release Tools\",\n\"version\": \"0.0.1\",\n\"publisher\": \"fabrikam\",\n\"targets\": [\n{\n\"id\": \"Microsoft.VisualStudio.Services\"\n}\n],    \"description\": \"Tools for building/releasing with Fabrikam. Includes one build/release task.\",\n\"categories\": [\n\"Azure Pipelines\"\n],\n\"icons\": {\n\"default\": \"images/extension-icon.png\"        },\n\"files\": [\n{\n\"path\": \"buildAndReleaseTask\"\n}\n],\n\"contributions\": [\n{\n\"id\": \"custom-build-release-task\",\n\"type\": \"ms.vss-distributed-task.task\",\n\"targets\": [\n\"ms.vss-distributed-task.tasks\"\n],\n\"properties\": {\n\"name\": \"buildAndReleaseTask\"\n}\n}\n]\n}\n</code></pre></li> <li>Replace the <code>\"publisher\"</code> with your publisherId<ul> <li>You may create a new publisherId from your publisher page in the Visual Studio Marketplace</li> <li>You just need to enter Name and PublisherId, everything else is optional</li> </ul> </li> <li>Replace also <code>\"name\"</code>, <code>\"description\"</code>, <code>\"files\"</code>/<code>\"path\"</code> and <code>\"contributions\"</code>/<code>\"properties\"</code>/<code>\"name\"</code></li> <li>Package the extension using the Azure DevOps TFS CLI:<ul> <li><code>tfs extension create --manifest-globs vss-extension.json</code></li> </ul> </li> <li>The file <code>{publisherId}.{extension_Id}.{extension_version}.vsix</code> will appear, containing your packaged extension! \ud83c\udf89</li> </ul>"},{"location":"cloud/azure-pipelines/#publish-the-task","title":"Publish the task","text":"<ul> <li>Publish and share the extension<ul> <li>Using the portal:<ul> <li>Back to the Visual Studio Marketplace portal &gt; go to Manage Publisher &amp; Extensions </li> <li>New extension &gt; Azure DevOps</li> <li>Upload your vsix file</li> <li>Once the upload is finished, there will be an automatic check</li> <li>The extension will be uploaded on the marketplace as Private by default<ul> <li>Obviously, it can be made Public at any time</li> </ul> </li> </ul> </li> <li>Using Powershell:<ul> <li>Get the OrganizationName (including dev.azure.com/)</li> <li>Create a Personal Access Token from Azure DevOps<ul> <li>Azure DevOps &gt; User settings &gt; Personal Access Tokens &gt; New Token</li> </ul> </li> <li>Share the extension<ul> <li><code>tfx extension publish --manifest-globs vss-extension.json --share-with dev.azure.com/ORGANIZATION_NAME</code></li> <li>It will prompt you for the Personal Access Token</li> <li>It will validate the extension and give you a feedback from your shell session</li> </ul> </li> </ul> </li> </ul> </li> <li>On Azure DevOps, go to Organization Settings &gt; Extensions &gt; Shared<ul> <li>Select the shared extension and click on Install<ul> <li>The extension will appear among the Installed ones</li> </ul> </li> </ul> </li> </ul>"},{"location":"cloud/azure-pipelines/#use-your-task","title":"Use your task","text":"<ul> <li>In Azure DevOps, go to your build/release pipeline</li> <li>Add task &gt; you can search for it by using the <code>\"name\"</code> parameter in the task.json file used to publish</li> </ul>"},{"location":"cloud/azure/","title":"azure","text":"<p>Resources</p> <ul> <li>Adam Marczak's  Azure for Everyone and especially its  Azure Fundamentals playlist</li> <li>John Savill's  Azure Masterclass</li> <li>The Azure Fundamentals Learning Path in  Microsoft Learn</li> </ul>"},{"location":"cloud/azure/#azure-vms","title":"Azure VMs","text":""},{"location":"cloud/azure/#set-up-a-spot-windows-server-2019-vm-with-the-azure-powershell-module","title":"Set up a spot Windows Server 2019 VM with the Azure Powershell Module","text":"<ul> <li>Install and import the Azure <code>Az</code> Powershell module in the current session <pre><code>Install-Module -Name Az -Force -Verbose\nImport-Module -Name Az\n</code></pre></li> <li>Connect to your Azure subscription using the browser <pre><code>Connect-AzAccount\n</code></pre></li> <li>Create the Resource Group and the VM; remember to set the <code>$region</code> and the <code>$user</code> variables. The password is prompted interactively. <pre><code>$ResourceGroup = 'dev-test'\n$region = 'westeurope'\n$passwd = ConvertTo-SecureString $(Read-Host \"Password\") -AsPlainText -Force\n$Credential = New-Object System.Management.Automation.PSCredential ($user, $passwd);\n# Create resource group\nNew-AzResourceGroup -Name $ResourceGroup -Location $region -Verbose\n# Create VM\nNew-AzVM -ResourceGroupName $ResourceGroup -Location $region -Name $ResourceGroup -Image Win2019Datacenter -Credential $Credential -Priority Spot -Verbose\n</code></pre></li> </ul>"},{"location":"cloud/docker/","title":"docker","text":"Resources <ul> <li> Docker docs! The starting point</li> <li> Docker and Kubernetes complete tutorial, a very detailed playlist from beginner to advanced level in both Docker and Kubernetes - it's also a  Udemy course, by the way</li> <li> Dev containers, a playlist from the VS Code YouTube channel about containerized dev environments</li> <li> Docker Mastery Udemy course on Docker and Kubernetes, by a Docker Captain</li> <li>15 Quick Docker Tips</li> </ul>"},{"location":"cloud/docker/#concepts","title":"Concepts","text":"<ul> <li>Docker takes advantage of the kernel's property of namespacing (isolating resources per process or group of processes, e.g. when a process needs a specific portion of the actual hardware such as the hard drive, but not the rest) and control groups (cgroups) (limiting the amount of resources - RAM, CPU, HD I/O, network bandwith, etc. per process or group of processes)</li> </ul> <ul> <li>So a container is basically a process whose system calls are redirected to a namespaced portion of dedicated hardware (HD, RAM, network, CPU, etc.) through the host's kernel</li> </ul> <ul> <li>An image is essentially a read-only filesystem snapshot with a startup command</li> </ul> <ul> <li>Docker's architecture:</li> </ul>"},{"location":"cloud/docker/#install-docker-on-debian","title":"Install docker on Debian","text":"<p>Reference  here)</p> <ul> <li>Compare your Debian version (in <code>/etc/issue</code>) with the current  installation requirements</li> <li>Set up the stable Docker repository with  <pre><code>sudo apt-get update\nsudo apt-get install ca-certificates curl gnupg lsb-release\nsudo curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\nsudo echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre></li> <li>Install the Docker engine: <pre><code>sudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io\n</code></pre></li> <li>Test the installation with: <pre><code>docker --version\n</code></pre></li> <li>Create a new user group called 'docker' and add your user to it: <pre><code>sudo groupadd docker\nsudo usermod -aG docker $USER\nnewgrp docker\n</code></pre> Test the last commands by running <code>docker</code> without having to preface <code>sudo</code>.</li> <li>Configure Docker to start on boot: <pre><code>sudo systemctl enable docker.service\nsudo systemctl enable containerd.service\n</code></pre></li> <li>The docker daemon configuration is stored in <code>~/.docker/config.json</code></li> </ul>"},{"location":"cloud/docker/#administration","title":"Administration","text":""},{"location":"cloud/docker/#images-and-containers","title":"Images and containers","text":"<ul> <li><code>docker image ls</code>: all the images in the docker host's cache</li> </ul> <p>Images are referred to with <code>USER/REPOSITORY:TAG</code></p> <p>\"Official\" images live at the 'root namespace' of the registry, therefore you can call them by the repository name without specifying the <code>USER</code> (es. <code>nginx:latest</code>)</p> <p>Tags are basically a specific image commit into its repository, similarly to Git. Multiple tags can refer to the same image ID.</p> <ul> <li> <p><code>docker image history nginx:latest</code>: list all the layers in the <code>nginx:latest</code> image (sorted by datetime descending)</p> </li> <li> <p><code>docker container ls -al</code>: all the containers with all the statuses (running, created, exited, stopped, etc)</p> </li> <li> <p><code>docker container run --detach --name postgres14 --publish 5432:5432 --env POSTGRES_USER=root --env POSTGRES_PASSWORD=secretpassword postgres:14-alpine</code>: runs a new detached instance of the <code>postgres:14-alpine</code> image by publishing the container's port <code>5432</code> (syntax is <code>--publish HOST:CONTAINER</code>) and using the specified environment variables</p> </li> <li> <p><code>docker container run --interactive --tty --name ubuntu ubuntu bash</code>: will overwrite the startup command included with the <code>ubuntu:latest</code> image with the <code>bash</code> command, thus opening an interactive pseudo-tty shell</p> </li> <li> <p><code>docker image prune --all</code>: deletes all  dangling images, check before deleting with <code>docker images --filter \"dangling=true\"</code></p> </li> </ul> <p>Info</p> <p>How to pull the image of a specific distro (es. Alpine) without specifying the tag version? ( to be tested): get all the tags of a specific <code>image</code> in a list (you will need the JSON processor jq, just use <code>apt-get install jq</code>) and filtering them by distro with <code>grep</code>: <pre><code>wget -q https://registry.hub.docker.com/v1/repositories/postgres/tags -O - | jq -r '.[].name' | grep '\\-alpine'\n</code></pre> Replace <code>postgres</code> with your image name</p> <ul> <li> <p><code>docker container cp FILE CONTAINER_NAME:/</code>: it copies <code>FILE</code> in the root folder of the <code>CONTAINER_NAME</code></p> </li> <li> <p><code>docker container stop CONTAINER_NAME_OR_ID</code>: it sends a <code>SIGTERM</code> signal to the primary process inside the container, letting it shut down on its own time and with its own clean-up procedure</p> </li> <li> <p><code>docker container kill CONTAINER_NAME_OR_ID</code>: it sends a <code>SIGKILL</code> signal to the primary process inside the container, shutting it down immediately; it's automatically used by the Docker Server if the container's process does not respond to the <code>docker stop</code> command within 10 seconds. </p> </li> <li> <p><code>docker system prune</code>: it removes all stopped containers, all networks not used, all dangling images, all build cache</p> </li> <li> <p>Get the docker image ID by its name (<code>IMAGE-NAME</code>): <pre><code>docker images --format=\"{{.Repository}} {{.ID}}\" |      # Reformat the output of 'docker images'\ngrep \"IMAGE-NAME\" |         # Find your image\ncut -d' ' -f2               # Cut the output and pick the ID\n</code></pre></p> </li> <li> <p><code>docker system df</code> gives you stats about the occupied space, like <pre><code>TYPE            TOTAL     ACTIVE    SIZE      RECLAIMABLE\nImages          8         0         1.665GB   1.665GB (100%)\nContainers      0         0         0B        0B\nLocal Volumes   6         0         205.1MB   205.1MB (100%)\nBuild Cache     0         0         0B        0B\n</code></pre></p> </li> <li> <p>Remove all Exited containers: it may occur that some containers with running processes are in the <code>Exited</code> status and therefore won't be deleted with the <code>docker container rm</code> command - or, the specific ID will be removed and immediately replaced with another one. Then just run: <pre><code>docker rm $(docker ps -a -f status=exited -q)\n</code></pre></p> </li> </ul>"},{"location":"cloud/docker/#network-drivers-and-aliases","title":"Network drivers and aliases","text":"<p>Reference  here.</p> <ul> <li><code>docker network ls</code>: all the networks in docker</li> </ul> <p> Remember that the <code>bridge</code> default network does not support the internal DNS - which you can find in any new bridge network created with <code>docker network create --driver bridge NETWORK</code>. So, it's a best practice to always create your custom networks and attach your containers to them (with <code>docker network connect NETWORK CONTAINER</code>). </p> <ul> <li> <p><code>docker network inspect --format \"{{json .Containers }}\" bridge | jq</code>: lists all the containers connected to the default docker network <code>bridge</code></p> </li> <li> <p><code>docker network inspect --format \"{{json .IPAM.Config }}\" bridge | jq</code> : gives the IP range used by the default docker network <code>bridge</code></p> </li> <li> <p><code>docker container inspect --format \"{{json .NetworkSettings.IPAddress }}\" nginx | jq</code> : the <code>nginx</code> container's internal IP address read from the <code>inspect</code> output ( use the container's hostname instead of IP address... containers really are ephemeral!)</p> </li> <li> <p><code>docker network connect NETWORK CONTAINER --alias ALIAS</code>: will set the <code>ALIAS</code> of the container in the network. </p> </li> </ul> <p>Multiple containers can even have the same alias. That's used for :material-wiki: DNS Round-Robin, a form of DNS-based load-balacing test.</p> <p>Using a <code>makefile</code> to speed up the docker commands</p> <p>To avoid typing long bash commands, automate the most usual ones with a Makefile (also a tutorial at Makefiletutorial). The Makefile follows the syntax: <pre><code>target: prerequisites\n    command\ncommand\ncommand\n</code></pre> Therefore an example of handling database migrations on a PostgreSQL container would be: <pre><code># Variables\ncontainername = YOUR-CONTAINER-NAME\ndbname = YOUR-DB-NAME\ndbuser = YOUR-DB-USER\ndbpassword = YOUR-DB-PASSWORD\n\nrunbash: # It opens a bash shell on the target container\nsudo docker exec -it $(containername) bash\n\nrunpostgres: # It runs a PostgreSQL container\nsudo docker run -d --name $(containername) -p 54325:5432 -e POSTGRES_DB=root -e POSTGRES_USER=$(dbuser) -e POSTGRES_PASSWORD=$(dbpassword) postgres:14-alpine\n\ncreatedb: # It creates the PostgreSQL database in the container\nsudo docker exec -it $(containername) createdb --username=$(dbuser) --owner=$(dbuser) $(dbname)\n\ndropdb: # It drops the PostgreSQL database in the container\nsudo docker exec -it $(containername) dropdb $(dbname)\n\nmigrateup: # It performs a forward db migration\nmigrate -path db/migrations -database \"postgresql://$(dbuser):$(dbpassword)@localhost:54325/$(dbname)?sslmode=disable\" -verbose up\n\nmigratedown: # It performs a backward db migration\nmigrate -path db/migrations -database \"postgresql://$(dbuser):$(dbpassword)@localhost:54325/$(dbname)?sslmode=disable\" -verbose down    \n\nrunpsql: # It opens a psql shell on the target container\nsudo docker exec -it $(containername) psql $(dbname)\n\n.PHONY: runbash runpostgres createdb dropdb runpsql migrateup migratedown\n</code></pre> Then quickly recall the commands with <code>make</code>! \ud83c\udf89\ud83c\udf8a For example: <pre><code>make runbash    # It will open a bash shell on the specified container\n</code></pre> For instance, to quickly set up your new containerized database, just: <pre><code>make runpostgres # Create the container and start it\nmake createdb # Create the database\nmake migrateup # Run the first migration to create the schema\nmake runpsql # Start the psql CLI\n</code></pre>  Watch out for tabs in the Makefile, as explained in  this StackOverflow answer. Use <code>cat -etv Makefile</code> to look for missing tabs (<code>^I</code>).</p>"},{"location":"cloud/docker/#troubleshooting","title":"Troubleshooting","text":"<p>Learn on a running container: <code>docker run -d IMAGE_NAME ping google.com</code>, where the <code>ping google.com</code> command overrides the default image's startup command and leaves the container always running</p> <p>Warning</p> <p>The error docker: Error response from daemon: driver failed programming external connectivity on endpoint ...: Error starting userland proxy: listen tcp4 0.0.0.0:5432: bind: address already in use means that the specified local port (i.e. in the example, the bind <code>0.0.0.0:5432</code>) is already used by another process... Just change the host port .</p> <ul> <li> <p><code>docker container exec --interactive --tty postgres14 psql -U root</code>: it will start a command (the one specified after the image's name, here <code>psql -U root</code>) running in addition to the startup command</p> </li> <li> <p><code>docker container logs CONTAINER_NAME_OR_ID</code>: it shows the logs of the specified <code>CONTAINER_NAME_OR_ID</code>. This is the same output as running the container without the <code>--detach</code> flag.</p> </li> </ul>"},{"location":"cloud/docker/#developing","title":"Developing","text":"<p>An example of Dockerfile for a Python application (references  here) with multiple 'stanzas'.</p> <p>Here you can find some  best practices for writing Dockerfile.</p> <pre><code># syntax=docker/dockerfile:1\n\n# Base image\nFROM python:3.8-slim-buster\n\n# Container's default location for all subsequent commands\nWORKDIR /app\n\n# Copy command from the Dockerfile local folder to the path relative to WORKDIR \nCOPY requirements.txt requirements.txt\n\n# Running a command with the image's default shell (it can be changed with a SHELL command)\nRUN pip3 install -r requirements.txt\n\n# Copy the whole source code\nCOPY . .\n\n# Command we want to run when our image is executed inside a container\n# Notice the \"0.0.0.0\" meant to make the application visible from outside of the container\nCMD [ \"python3\", \"-m\" , \"flask\", \"run\", \"--host=0.0.0.0\"]\n</code></pre> <ul> <li> <p><code>CMD</code> arguments can be over-ridden: <pre><code>cat Dockerfile\nFROM ubuntu\nCMD [\"echo\"]\n$ docker run imagename echo hello\nhello\n</code></pre> <code>ENTRYPOINT</code> arguments can NOT be over-ridden: <pre><code>cat Dockerfile\nFROM ubuntu\nENTRYPOINT [\"echo\"]\n$ docker run imagename echo hello\necho hello\n</code></pre></p> </li> <li> <p>To install an unpacked service using its executable on Docker, use the following Dockerfile: <pre><code>FROM mcr.microsoft.com/windows/servercore:ltsc2019\nWORKDIR /app\nCOPY . \"C:/app\"\nRUN [\"C:/Windows/Microsoft.NET/Framework/v4.0.30319/InstallUtil.exe\", \"/i\", \"EXECUTABLE_NAME.exe\"]\nSHELL [\"powershell\", \"-Command\", \"$ErrorActionPreference = 'Stop'; $ProgressPreference = 'SilentlyContinue';\"]\nCMD c:\\app\\Wait-Service.ps1 -ServiceName 'SERVICE_NAME' -AllowServiceRestart\n</code></pre> Where the <code>Wait-Service.ps1</code> script is  here.</p> </li> </ul>"},{"location":"cloud/oracle/","title":"oracle cloud","text":"<p>Resources</p> <ul> <li>Launch Always Free Resources with Terraform</li> <li>Oracle Cloud Free Tier</li> <li>Oracle Cloud Always Free Resources</li> </ul>"},{"location":"cloud/oracle/#installation","title":"Installation","text":"<p>Install with interactive installation: <pre><code>sudo bash -c \"$(curl -L https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.sh)\" </code></pre> While responding to the installation prompts, keep all the installed files and folders in the same path (e.g. your <code>$HOME</code>) for consistency.</p> <p>Test your installation: <pre><code>oci --version\n</code></pre></p> <p>Configure the CLI with <code>oci setup config</code>; the prompts will ask for </p> <ul> <li>your <code>.oci/config</code> file location</li> <li>your user's OCID, tenancy's OCID, API Key fingerprint and region</li> </ul> <p>Test the connection with <pre><code>oci os ns get   # It returns the tenancy's namespaces\n</code></pre></p> <p>Error</p> <p>Troubleshoot the <code>NotAuthenticated</code> error with this blog post.</p>"},{"location":"cloud/oracle/#check-shape-availability","title":"Check shape availability","text":"<p>Get the list of compartments with <pre><code>oci iam compartment list\n</code></pre></p> <p>Info</p> <p>The output of the CLI are JSON records (readability is improved using <code>--output table</code>). Save them in a variable with <code>VARIABLENAME=$(oci ...)</code> then parse them in bash with jq using <code>echo $VARIABLENAME | jq '.'</code></p> <p>Get the list of compartments' OCIDs into an array:</p> <pre><code>COMPARTMENTS=$(oci iam compartment list)\necho $COMPARTMENTS &gt; compartments.json\nCOMPARTMENTS_OCID=$(cat compartments.json | jq -r '.data[].id')\nCOMPARTMENT_ARRAY=($(echo $COMPARTMENTS_OCID | tr \" \" \"\\n\"))\nrm compartments.json\n</code></pre> <p>You can access them with a <code>for</code> loop like:</p> <pre><code>for compartment_ocid in \"${COMPARTMENT_ARRAY[@]}\"\ndo\necho $compartment_ocid\ndone\n</code></pre> <p>Using the same <code>for</code> loop as above, get the list of available instance shapes (reference). The <code>grep</code> part is used to check if a specific shape is available in each compartment:</p> <pre><code>for compartment_ocid in \"${COMPARTMENT_ARRAY[@]}\"\ndo\nSHAPE=$(oci compute shape list --compartment-id $compartment_ocid | grep \"VM.Standard.A1.Flex\")\n# Check if shape is available\nif [ -z \"$SHAPE\" ]\nthen\necho \"No shape found in compartment $compartment_ocid\"\nelse\necho $SHAPE\nfi\ndone\n</code></pre> <p> Next: create a .NET worker service with the .NET SDK (documentation also available  here)</p>"},{"location":"cloud/vagrant/","title":"vagrant","text":""},{"location":"cloud/vagrant/#initialization-errors","title":"Initialization errors","text":"<ul> <li> When you <code>vagrant up BOX_NAME</code><ul> <li>you may get the error <code>The \"metadata.json\" file for the box 'BOX_NAME' was not found.</code>. As per [ this post], you need to: <pre><code>cd ~/.vagrant.d/boxes\nrm -f BOX_NAME\n</code></pre></li> <li>If you get <code>A Vagrant environment or target machine is required to run this command.</code>... you're just not in the correct path! <code>cd</code> into  your <code>vagrantfile</code> folder.</li> </ul> </li> </ul>"},{"location":"cloud/windows/","title":"Windows","text":""},{"location":"cloud/windows/#windows-subsystems-for-linux-wsl-2","title":"Windows Subsystems for Linux (WSL) 2","text":"<p>Info</p> <p>TBD</p> <p>Warning</p> <p>Always use a Powershell session as Administrator!</p>"},{"location":"cloud/windows/#on-windows-11-windows-10-2004-build-19041","title":"On Windows 11 / Windows 10 2004 build &gt;= 19041","text":""},{"location":"cloud/windows/#on-windows-10-1903-build-18362","title":"On Windows 10 1903 build &gt;= 18362","text":""},{"location":"cloud/windows/#to-be-ordered","title":"To be ordered","text":"<ul> <li>Activate WSL<ul> <li>On Windows 10 build &gt;= 19041 or Windows 11: <code>wsl --install</code></li> <li>On previous versions of Windows:  <pre><code>dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\n</code></pre></li> </ul> </li> <li>Upgrade to WSL2 and set default version<ul> <li>On Windows 10 build &gt;= 19041 or Windows 11: automatically handled by the previous command</li> <li>On previous versions of Windows:</li> </ul> </li> <li> <p>Solve DNS issue</p> <ul> <li>Procedure: https://superuser.com/questions/1533291/how-do-i-change-the-dns-settings-for-wsl2</li> <li>Also helpful: https://www.ricmedia.com/set-permanent-dns-nameservers-ubuntu-debian-resolv-conf/</li> <li>Watch out the firewall!</li> </ul> </li> <li> <p>Create a SSH key pair on Windows</p> </li> </ul>"},{"location":"db/mysql/","title":"mysql","text":"<ul> <li>[material-stack-overflow] In case of error similar to:</li> </ul> <pre><code>2023-01-22 20:41:05 0 [ERROR] mysqld.exe: Aria recovery failed. Please run aria_chk -r on all Aria tables and delete all aria_log.######## files\n...\n2023-01-22 20:41:05 0 [ERROR] Could not open mysql.plugin table. Some plugins may be not loaded\n2023-01-22 20:41:05 0 [ERROR] Failed to initialize plugins.\n2023-01-22 20:41:05 0 [ERROR] Aborting\n</code></pre> <p>Remove (or rename) the following files in the MySQL root folder (in this case <code>C:\\XAMPP\\MySql\\data</code>, in Linux in <code>/var/lib/mysql</code>):</p> <pre><code>ib_logfile0\nib_logfile1\naria_log_control\n</code></pre> <p>Then restart the service.</p>"},{"location":"db/postgres/","title":"postgresql","text":"<p>Resources</p> <ul> <li> Backend master class with Golang, Postgres and Docker </li> </ul>"},{"location":"db/postgres/#install","title":"Install","text":"<ul> <li> <p>Here's the installation tutorial</p> </li> <li> <p>To get started with Python, use the psycopg driver: <pre><code>sudo apt install python3-dev libpq-dev\npip install psycopg2\n</code></pre></p> </li> </ul>"},{"location":"db/postgres/#getting-started-commands","title":"Getting started commands","text":"<ul> <li> <p>To send a command directly from bash, <code>sudo u postgres psql -c \"SELECT 1\"</code> </p> </li> <li> <p>To start PostgreSQL CLI psql, <code>sudo -u postgres psql</code></p> <ul> <li>To list databases, <code>\\l</code></li> <li>To choose a database, <code>\\c DATABASE_NAME</code></li> <li>To show all the tables in the database, <code>\\dt</code></li> <li>To look for a specific table in the database, <code>\\dt *PATTERN*</code></li> <li>To create a database, <code>CREATE DATABASE your_db_name;</code></li> </ul> </li> <li> <p>To test a connection to a specific database, use pg_isready: <pre><code>pg_isready -d &lt;db_name&gt; -h &lt;host_name&gt; -p &lt;port_number&gt; -U &lt;db_user&gt; </code></pre> The command <code>echo $?</code> will return the exit code of <code>pg_isready</code>, i.e.</p> <ul> <li><code>0</code> = the server is accepting connections normally</li> <li><code>1</code> = the server is rejecting connections (e.g. during startup)</li> <li><code>2</code> = there was no response to the connection attempt</li> <li><code>3</code> = no attempt was made (for example due to invalid parameters).</li> </ul> </li> </ul> <p>To test a login, try <code>psql -d \"postgresql://USER:PASSWORD@HOST:PORT/t\" -c \"select now()\"</code>.</p>"},{"location":"db/postgres/#create-a-database","title":"Create a database","text":"<p>Log in to Postgres <pre><code>sudo su - postgres\npsql\n</code></pre> Create a user with password and database and grant permission on the db itself: <pre><code>create user hero;\ncreate database my_db;\nalter role hero with password 'my_db@123';\ngrant all privileges on database my_db to hero;\nalter database my_db owner to hero;\n</code></pre></p>"},{"location":"db/postgres/#reset-the-users-password","title":"Reset the user's password","text":"<p>Enter without password: <pre><code>sudo -u postgres psql\n</code></pre> Then <pre><code>ALTER USER user_name WITH PASSWORD 'new_password';\n</code></pre></p>"},{"location":"db/postgres/#random-notes","title":"Random notes","text":"<ul> <li>A <code>bigserial</code> type of number is a \"big (8byte/64bit) autoincrementing integer\"</li> </ul>"},{"location":"db/postgres/#about-db-schema","title":"About db schema","text":"<ul> <li>In dbdiagram (a quick memo about the syntax in this holistics.io blog post), the following schema instructions: <pre><code>// Accounts\nTable accounts as A {\n  id bigserial [pk]\n  owner varchar [not null]\n  balance bigint [not null]\n  currency varchar [not null]\n  created_at timestamptz [not null, default: `now()`]\n\n  // Just list for owner\n  Indexes {\n    owner\n  }\n}\n\n// Entries per account\nTable entries {\n  id bigserial [pk]\n  account_id bigint [ref: &gt; A.id]\n  amount bigint [not null, note: 'it can be negative or positive']\n  created_at timestamptz [not null, default: `now()`]\n\n  // Just list for account\n  Indexes {\n    account_id\n  }\n}\n\n// Transfers between accounts\nTable transfers {\n  id bigserial [pk]\n  from_account_id bigint [ref: &gt; A.id]\n  to_account_id bigint [ref: &gt; A.id]\n  amount bigint [not null, note: 'it must be positive']\n  created_at timestamptz [not null, default: `now()`]\n\n  // List for sender, receiver or both\n  Indexes {\n    from_account_id\n    to_account_id\n    (from_account_id, to_account_id) // Composite index\n  }\n}\n</code></pre> Becomes <pre><code>CREATE TABLE \"accounts\" (\n\"id\" bigserial PRIMARY KEY,\n\"owner\" varchar NOT NULL,\n\"balance\" bigint NOT NULL,\n\"currency\" varchar NOT NULL,\n\"created_at\" timestamptz NOT NULL DEFAULT (now())\n);\nCREATE TABLE \"entries\" (\n\"id\" bigserial PRIMARY KEY,\n\"account_id\" bigint,\n\"amount\" bigint NOT NULL,\n\"created_at\" timestamptz NOT NULL DEFAULT (now())\n);\nCREATE TABLE \"transfers\" (\n\"id\" bigserial PRIMARY KEY,\n\"from_account_id\" bigint,\n\"to_account_id\" bigint,\n\"amount\" bigint NOT NULL,\n\"created_at\" timestamptz NOT NULL DEFAULT (now())\n);\nALTER TABLE \"entries\" ADD FOREIGN KEY (\"account_id\") REFERENCES \"accounts\" (\"id\");\nALTER TABLE \"transfers\" ADD FOREIGN KEY (\"from_account_id\") REFERENCES \"accounts\" (\"id\");\nALTER TABLE \"transfers\" ADD FOREIGN KEY (\"to_account_id\") REFERENCES \"accounts\" (\"id\");\nCREATE INDEX ON \"accounts\" (\"owner\");\nCREATE INDEX ON \"entries\" (\"account_id\");\nCREATE INDEX ON \"transfers\" (\"from_account_id\");\nCREATE INDEX ON \"transfers\" (\"to_account_id\");\nCREATE INDEX ON \"transfers\" (\"from_account_id\", \"to_account_id\");\nCOMMENT ON COLUMN \"entries\".\"amount\" IS 'it can be negative or positive';\nCOMMENT ON COLUMN \"transfers\".\"amount\" IS 'it must be positive';\n</code></pre> Notice the <code>bigserial</code> number type and the <code>now()</code> function in the dbdiagram syntax, which are specifically selected to be exported to PostgreSQL.</li> </ul>"},{"location":"db/postgres/#deploy-on-docker","title":"Deploy on  Docker","text":"<ul> <li> <p>All the steps to set up a running <code>postgres</code> Docker container are exhaustively explained in the description of the official postgres image on Docker Hub.</p> </li> <li> <p>To run *.sql scripts on a postgres container, </p> <ol> <li>Copy your <code>SCRIPT.sql</code> file to the <code>CONTAINER_NAME</code> running container's root: <pre><code>docker container cp SCRIPT.sql CONTAINER_NAME:/\n</code></pre></li> <li>Verify that the file is there: <pre><code>docker container exec -it CONTAINER_NAME bash\n</code></pre> Then <code>ls SCRIPT*</code></li> <li>Instruct the <code>psql</code> client to run the file you just copied as the default username <code>root</code> on the database <code>your_database_name</code> <pre><code>docker container exec -it CONTAINER_NAME psql --dbname=your_database_name --username root -f /SCRIPT.sql\n</code></pre></li> </ol> </li> </ul> <p>Warning</p> <p>This whole sequence could probably be bypassed by using a Dockerfile</p>"},{"location":"db/postgres/#migrations","title":"Migrations","text":"<p>To easily perform migrations on PostgreSQL, you may use the  golang-migrate CLI tool, written in :language-go: Go. </p> <pre><code># Install golang-migrate\nbrew install golang-migrate\n\n# Test installation\nmigrate -version\n\n# Create the db migrations folder in your project folder\nmkdir -p db/migrations\n\n# Create your first migration\nmigrate create -ext sql -dir db/migrations -seq initial_schema\n</code></pre> <p>The last command will create the first migration called <code>initial_schema</code> in files with <code>.sql</code> extension, in the path specified by <code>-dir</code> and with a sequential (<code>-seq</code>) number to keep track of progressing migrations. The first two files are:</p> <pre><code>./db/migrations/000001_initial_schema.up.sql      # Script to \"migrate up\", i.e. moving forward in migrations \n./db/migrations/000001_initial_schema.down.sql    # Script to \"migrate down\", i.e. moving backwards in migrations\n</code></pre> <p>If you wish to do it manually, copy your schema creation script in the first file, and edit the second file with the reverse commands (i.e. dropping every object in the database).</p> <p>To run your first migration, use:</p> <pre><code>migrate -path ./db/migrations -database \"postgresql://USERNAME:PASSWORD@HOSTNAME:PORT/DATABASE_NAME?PARAMETERS\" -verbose up\n</code></pre> <p>Where: * the <code>-database</code> switch requires the whole  database URL * the <code>up</code> argument specifies the direction of the migration * the <code>PARAMETERS</code> (more specifically <code>?PARAMETER1=VALUE1&amp;PARAMETER2=VALUE2</code>) can be used to add any additional parameter</p> <p>Warning</p> <p>If you encounter the SSL is not enabled on the server error (like in a Docker PostgreSQL container), use the <code>?sslmode=disable</code> parameter after the <code>DATABASE_NAME</code> in the URL.</p> <p>The first migration (containing the schema initialization scripts) will also add the <code>schema_migrations</code> table in the database.</p>"},{"location":"db/sqlserver/","title":"Microsoft SQL Server","text":"<p>The reference repository is  sannae/tsql-queries.</p>"},{"location":"db/sqlserver/#about-database-administration","title":"About database administration","text":""},{"location":"db/sqlserver/#resources-services-and-processes","title":"Resources, services and processes","text":"<ul> <li> <p>Name of the service and name of the instance: <pre><code>select @@servername     -- Hostname and instance name\nselect @@servicename    -- Instance service name\n</code></pre></p> </li> <li> <p> List all the processes with an open connection to the current instance:</p> </li> </ul> <pre><code>SELECT hostname, COUNT(hostname) AS Processes\nFROM sys.sysprocesses AS P\nJOIN sys.sysdatabases AS D ON (D.dbid = P.dbid)\nJOIN sys.sysusers AS U ON (P.uid = U.uid)\nGROUP BY hostname\nORDER BY COUNT(hostname) DESC\n</code></pre> <ul> <li> Show the currently allocated physical memory:</li> </ul> <pre><code>-- The following queries return information about currently allocated memory.\nSELECT\n(total_physical_memory_kb/1024) AS Total_OS_Memory_MB,\n(available_physical_memory_kb/1024)  AS Available_OS_Memory_MB\nFROM sys.dm_os_sys_memory;\n\nSELECT  (physical_memory_in_use_kb/1024) AS Memory_used_by_Sqlserver_MB,  (locked_page_allocations_kb/1024) AS Locked_pages_used_by_Sqlserver_MB,  (total_virtual_address_space_kb/1024) AS Total_VAS_in_MB,\nprocess_physical_memory_low,  process_virtual_memory_low  FROM sys.dm_os_process_memory; -- The following query returns information about current SQL Server memory utilization.\nSELECT\nsqlserver_start_time,\n(committed_kb/1024) AS Total_Server_Memory_MB,\n(committed_target_kb/1024)  AS Target_Server_Memory_MB\nFROM sys.dm_os_sys_info;\n</code></pre> <ul> <li> It returns infos about the size of the database and the corresponding objects (tables, rows, etc.). On SQL Server: <pre><code>/* SQL Server */\n-- Returns database Name, Log Size, Row Size, Total Size for current db\nSELECT [Database Name] = DB_NAME(database_id)\n, [Log Size (MB)] = CAST(SUM(CASE WHEN type_desc = 'LOG' THEN size END) * 8./1024 AS DECIMAL(8,2))\n, [Row Size (MB)] = CAST(SUM(CASE WHEN type_desc = 'ROWS' THEN size END) * 8./1024 AS DECIMAL(8,2))\n, [Total Size (MB)] = CAST(SUM(size) * 8. / 1024 AS DECIMAL(8,2))\nFROM sys.master_files WITH(NOWAIT)\nWHERE database_id = DB_ID() -- for current db \nGROUP BY database_id\n</code></pre> And on MySQL: <pre><code>/* MySql */\n-- Returns the database sizes in MB\nSELECT table_schema AS \"Database\", ROUND(SUM(data_length + index_length) / 1024 / 1024, 2) AS \"Size (MB)\" FROM information_schema.TABLES GROUP BY table_schema;\n\n-- Returns the table of a specific DATABASE_NAME\nSELECT table_name AS \"Table\",\nROUND(((data_length + index_length) / 1024 / 1024), 2) AS \"Size (MB)\"\nFROM information_schema.TABLES\nWHERE table_schema = \"database_name\"\nORDER BY (data_length + index_length) DESC;\n</code></pre></li> </ul>"},{"location":"db/sqlserver/#users-and-authentication","title":"Users and authentication","text":"<ul> <li> It activates the  Mixed Mode Authentication in the current SQL Server instance:</li> </ul> <p><pre><code>USE [master]\nGO\n/* [Note: 2 indicates mixed mode authentication. 1 is for windows only authentication] */\nEXEC xp_instance_regwrite N'HKEY_LOCAL_MACHINE', N'Software\\Microsoft\\MSSQLServer\\MSSQLServer', N'LoginMode', REG_DWORD, 2\nGO\n</code></pre> \u26a0\ufe0f Remember to restart the SQL Server engine service!</p> <ul> <li> It lists all SQL server users, specifying the corresponding roles:</li> </ul> <pre><code>SELECT spU.name, MAX(CASE WHEN srm.role_principal_id = 3 THEN 1 END) AS sysadmin\n,MAX(CASE WHEN srm.role_principal_id = 4 THEN 1 END) AS securityadmin\n,MAX(CASE WHEN srm.role_principal_id = 5 THEN 1 END) AS serveradmin\n,MAX(CASE WHEN srm.role_principal_id = 6 THEN 1 END) AS setupadmin\n,MAX(CASE WHEN srm.role_principal_id = 7 THEN 1 END) AS processadmin\n,MAX(CASE WHEN srm.role_principal_id = 8 THEN 1 END) AS diskadmin\n,MAX(CASE WHEN srm.role_principal_id = 9 THEN 1 END) AS dbcreator\n,MAX(CASE WHEN srm.role_principal_id = 10 THEN 1 END) AS bulkadmin\nFROM sys.server_principals AS spR\nJOIN sys.server_role_members AS srm ON spR.principal_id = srm.role_principal_id\nJOIN sys.server_principals AS spU ON srm.member_principal_id = spU.principal_id\nWHERE spR.[type] = 'R' GROUP BY spU.name\n</code></pre> <ul> <li> It creates a new sysadmin login with the specified password:</li> </ul> <pre><code>USE DATABASE_NAME\n-- Creates new login\nCREATE LOGIN YOUR_USERNAME WITH PASSWORD = 'YOUR_PASSWORD';\nGO\n-- Assigns the sysadmin server role\nALTER SERVER ROLE sysadmin ADD MEMBER YOUR_USERNAME ;  GO </code></pre> <ul> <li>Find any object from its description with:</li> </ul> <pre><code>USE [YOUR_DATABASE]\nSelect [name] as ObjectName, Type as ObjectType\nFrom Sys.Objects\nWhere 1=1\nand [Name] like '%YOUR_OBJECT_DESCRIPTION%'\n</code></pre> <p>Object Types acronyms and names are listed in  this MS Learn article.</p>"},{"location":"db/sqlserver/#data-and-log-files","title":"Data and log files","text":"<ul> <li> <p> It restores a backup set from a bak file in the current SQL Server instance, also moving the corresponding files (i.e. basically restoring to a new location, you can find a good guide on this  Microsoft Docs page). </p> <ul> <li>Start by getting the logical names of the data and log files. The following <code>RESTORE</code> statement cannot be embedded into a <code>SELECT</code>, although you can use it to <code>INSERT</code> the values in a temporary table (following  this answer on Stack Overflow) <pre><code>-- Get logical names\nRESTORE FILELISTONLY FROM DISK='C:\\MY\\PATH\\TO\\BAK\\FILE.bak' WITH FILE=1\n</code></pre></li> <li>Then perform the restore with the option <code>MOVE</code> to replace the original data and log paths with new ones, if required: <pre><code>-- Restore database\nRESTORE DATABASE YOUR_DATABASE FROM DISK='C:\\MY\\PATH\\TO\\BAK\\FILE.bak'\nWITH MOVE YOUR_DATA_LOGICAL_NAME TO 'C:\\MY\\NEW\\PATH\\TO\\MDF\\FILE.mdf',\nMOVE YOUR_LOG_LOGICAL_NAME TO 'C:\\MY\\NEW\\PATH\\TO\\LDF\\FILE.ldf',\nRECOVERY, REPLACE, STATS = 10;\n</code></pre></li> </ul> </li> <li> <p> Returns a list of all the logical and physical names for the files of every database in the current SQL Server instance. Source here.</p> </li> </ul> <pre><code>SELECT d.name DatabaseName, f.name LogicalName,\nf.physical_name AS PhysicalName,\nf.type_desc TypeofFile\nFROM sys.master_files f\nINNER JOIN sys.databases d ON d.database_id = f.database_id\nGO\n</code></pre> <ul> <li>Migrating a SQL Server database to a lower version is not supported, in any version of SQL Server. You may want to consider generating scripts for the whole database schema and the data to be executed on the older-version instance.</li> </ul>"},{"location":"db/sqlserver/#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p> Isolate the database (i.e. put it in \"single user mode\") from any connection in order to perform maintenance tasks:     <pre><code>USE YOUR_DATABASE\nGO\nALTER DATABASE YOUR_DATABASE\nSET SINGLE_USER\nWITH ROLLBACK IMMEDIATE\nGO\n</code></pre>     Then do all your operations, and finally:     <pre><code>-- Set the database back in to multiple user mode\nUSE YOUR_DATABASE\nGO\nALTER DATABASE YOUR_DATABASE SET MULTI_USER\nGO\n</code></pre></p> </li> <li> <p> Retrieves the SQL Server Error Log: it returns the error log path for the current SQL Server instance (guide here):</p> <p><pre><code>USE master\nGO\nEXEC xp_readerrorlog 0, 1, N'Logging SQL Server messages in file'\nGO\n</code></pre> If the connection to SQL Server is not available, you may find the error log with the following options:</p> <ol> <li>SQL Server Configuration Manager &gt; SQL Server Services &gt; SQL Server (INSTANCE_NAME) &gt; Properties &gt; Startup Parameters &gt; add <code>-e</code></li> <li>Open regedit &gt; Go to <code>Computer\\HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Microsoft SQL Server\\(version)\\MSSQLServer\\Parameters</code></li> </ol> </li> <li> <p> Check out any condition with <code>NULL</code> by using <code>IS NULL</code> instead of <code>=</code> (and similarly for <code>IS NOT NULL</code> and <code>&lt;&gt;</code>)! This is especially true with SQL Server: <pre><code>/* NULL cheat sheet */\nDECLARE @MyNullVariable nvarchar(1)\nDECLARE @MyNonNullVariable nvarchar(1)\nSET @MyNullVariable = NULL\nSET @MyNonNullVariable = '1'\n\nIF (@MyNullVariable = NULL) PRINT 'True' ELSE PRINT 'False' -- Returns FALSE\nIF (@MyNullVariable IS NULL) PRINT 'True' ELSE PRINT 'False' -- Returns TRUE\nIF (@MyNullVariable &lt;&gt; NULL) PRINT 'True' ELSE PRINT 'False' -- Returns FALSE\nIF (@MyNonNullVariable IS NOT NULL) PRINT 'True' ELSE PRINT 'False' -- Returns TRUE\nIF (@MyNonNullVariable &lt;&gt; NULL) PRINT 'True' ELSE PRINT 'False' -- Returns FALSE\n</code></pre></p> </li> <li> <p>\u26a0\ufe0f Error SQL71564: Error validating element [YOUR_USER]: The element [YOUR_USER] has been orphaned from its login and cannot be deployed - means that the user specified in [YOUR_USER] is orphaned, i.e. does not have a corresponding login object, and this can occur even if there actually is a login whose GUID is matching the user's GUID.</p> </li> </ul> <p>So first of all, list the orphaned users:</p> <pre><code>EXEC sp_change_users_login 'Report'\n</code></pre> <p>If you already have a login id and password for this user, fix it by doing:</p> <pre><code>EXEC sp_change_users_login 'Auto_Fix', 'YOUR_USER'\n</code></pre> <p> Here's a GitHub Gist to fix all the orphaned users.</p> <ul> <li>\u26a0\ufe0f Error SQL71654: Error validating element [YOUR_ELEMENT]: the element [YOUR_ELEMENT] cannot be deployed as the script body is encrypted - in this case the database element [YOUR_ELEMENT] has been encrypted with TDE - i.e. <code>WITH ENCRYPTION</code>. Find the element and check if you can retrieve the encryption, or delete it.</li> </ul>"},{"location":"db/sqlserver/#browsing-data-and-tables","title":"Browsing data and tables","text":"<ul> <li> It searches a table in the specified DATABASE_NAME by looking for PATTERN in table name</li> </ul> <pre><code>USE DATABASE_NAME SELECT * FROM information_schema.tables WHERE Table_name LIKE '%PATTERN%'\n</code></pre> <ul> <li> It lists the general properties (rows, total occupied space, total free space, etc.) of all the tables in a specified DATABASE_NAME</li> </ul> <pre><code>USE DATABASE_NAME\nSELECT t.NAME AS TableName,\ns.Name AS SchemaName,\np.rows AS RowCounts,\nSUM(a.total_pages) * 8 AS TotalSpaceKB, CAST(ROUND(((SUM(a.total_pages) * 8) / 1024.00), 2) AS NUMERIC(36, 2)) AS TotalSpaceMB,\nSUM(a.used_pages) * 8 AS UsedSpaceKB, CAST(ROUND(((SUM(a.used_pages) * 8) / 1024.00), 2) AS NUMERIC(36, 2)) AS UsedSpaceMB, (SUM(a.total_pages) - SUM(a.used_pages)) * 8 AS UnusedSpaceKB,\nCAST(ROUND(((SUM(a.total_pages) - SUM(a.used_pages)) * 8) / 1024.00, 2) AS NUMERIC(36, 2)) AS UnusedSpaceMB\nFROM sys.tables t\nINNER JOIN sys.indexes i ON t.OBJECT_ID = i.object_id\nINNER JOIN sys.partitions p ON i.object_id = p.OBJECT_ID AND i.index_id = p.index_id\nINNER JOIN sys.allocation_units a ON p.partition_id = a.container_id\nLEFT OUTER JOIN sys.schemas s ON t.schema_id = s.schema_id\nWHERE t.NAME NOT LIKE 'dt%' AND t.is_ms_shipped = 0 AND i.OBJECT_ID &gt; 255 GROUP BY t.Name, s.Name, p.Rows\nORDER BY t.Name\n</code></pre> <ul> <li> <p> Create a +temporary table* with the result of two other <code>SELECT</code>s on other tables: <pre><code>-- Total n.1\nDECLARE @Total1 NVARCHAR(100)\nSET @Total1 = (SELECT COUNT(*) AS [Total1] FROM Table1)\n-- Total n.2\nDECLARE @Total2 NVARCHAR(100)\nSET @Total2 = (SELECT COUNT(*) AS [Total2] FROM Table2)\n-- Summary table\nDECLARE @Totals TABLE (TableNumber NVARCHAR(100), Total INT)\nINSERT INTO @Totals VALUES\n('Table1',@Total1),\n('Table2',@Total2)\nSELECT * FROM @Totals\n</code></pre></p> </li> <li> <p>In case you keep seeing the <code>Invalid object name</code> error in the query editor, even if SSMS properly shows the objects you're browsing, try  refreshing the IntelliSense cache.</p> </li> </ul>"},{"location":"db/sqlserver/#triggers-and-automation","title":"Triggers and automation","text":"<ul> <li> <p>To create an INSERT trigger, follow this template: <pre><code>USE DATABASENAME\nGO\n\n-- Start transaction\nBEGIN TRAN\nGO\n\n-- Create trigger\nCREATE TRIGGER [dbo].[T_TableName_TriggerName] ON TableName\nAFTER INSERT\nAS\nBEGIN\n/* Trigger body */\nEND\nGO\n\n-- Disable trigger after creation\nDISABLE TRIGGER [dbo].[T_TableName_TriggerName] ON TableName\n\n-- Commit transaction\nCOMMIT\nGO\n</code></pre> The trigger can then be enabled manually on SSMS, or by using <pre><code>ENABLE TRIGGER [dbo].[T_TableName_TriggerName] ON TableName\n</code></pre></p> </li> <li> <p>In the trigger body, the records inserted at each transaction are accessible through the virtual table <code>INSERTED</code>. Here's an example on how to reach for the values just inserted, i.e. the ones activating the trigger: <pre><code>    /* This is the trigger body */\n/* The following example copies some values of the record inserted in TableName directly into DestinationTable */\n\n-- SET NOCOUNT ON added to prevent extra result sets from\n-- interfering with SELECT statements.\nSET NOCOUNT ON;\n\n-- Select the record from TableName and add it to DestinationTable\nINSERT INTO DestinationTable\n(field1, field2, ...)\nSELECT\nTableName.Field1, TableName.Field2, ...\nFROM INSERTED\n-- Some other conditions like JOIN or WHERE\n</code></pre></p> </li> <li> <p>The same applies for a DELETE trigger (<code>CREATE TRIGGER [dbo].[T_TableName_TriggerName] ON TableName AFTER DELETE</code>): the deleted record are accessible by the trigger from the <code>DELETED</code> table</p> </li> </ul>"},{"location":"dev/git/","title":"git","text":"<p>Resources</p> <ul> <li>Official documentation in  Git docs.</li> <li> Atlassian's tutorials.</li> <li> Git for professionals by FreeCodeCamp</li> <li> Advanced Git by FreeCodeCamp</li> </ul>"},{"location":"dev/git/#most-used-commands","title":"Most used commands","text":"<ul> <li><code>git add FILENAME(S)</code>: it adds the files in \"stage\", i.e. preparing to commit</li> <li><code>git commit -m \"MESSAGE\"</code>: it does the commit (i.e. basically saving) the staged changes in the local repository </li> <li><code>git push</code>: it pushes the last commit towards the remote repository</li> <li><code>git pull</code>: it updates your current HEAD branch with the latest diffs from remote</li> </ul>"},{"location":"dev/git/#used-branches","title":"Used branches","text":"<ul> <li><code>git branch</code>: it shows the list of current branches</li> </ul>"},{"location":"dev/git/#create-a-new-branch","title":"Create a new branch:","text":"<ul> <li><code>git checkout -b NEWBRANCH</code> : it moves towards the newly created branch called NEWBRANCH</li> <li>Some best practices for naming the new branches  here:<ul> <li>Use grouping tokens (words) at the beginning of your branch names. For example:      <code>Test/DESCRIPTION</code>  : For test branches     <code>New/DESCRIPTION</code>   : For new features branches     <code>Bug/DESCRIPTION</code>   : For bugfixes branches     <code>Exp/DESCRIPTION</code>   : Experimental: for throwaway branch, to be trashed     <code>Verified/DESCRIPTION</code>  : For verified branches, to be merged to main</li> </ul> </li> <li><code>git push --set-upstream origin NEWBRANCH</code>: it creates the remote branch from the local one, pushing to it as well; as epxlained here</li> </ul>"},{"location":"dev/git/#pull-from-a-different-remote-branch","title":"Pull from a different remote branch","text":"<ul> <li>Check the remote branches (<code>git branch -r</code>) and the local branches (<code>git branch</code>)</li> <li>Switch to a specified branch (<code>git switch BRANCH_NAME</code>)</li> </ul>"},{"location":"dev/git/#publish-a-local-repo-to-a-new-remote-repo","title":"Publish a local repo to a new remote repo","text":"<p> Requirement: the remote repo must exist, otherwise it will return the error Repository not found</p> <ul> <li><code>git remote add origin https://github.com/.../REPONAME.git</code>: (right after the first <code>commit</code>) it defines the upstream origin. It will ask the credentials: <pre><code>    username: [insert USERNAME]\n    password: [insert PERSONAL ACCESS TOKEN]\n</code></pre></li> <li><code>git push -u origin master</code>: pusha il branch master sull'origin, da qui in poi baster\u00e0 usare <code>git push</code></li> </ul>"},{"location":"dev/git/#delete-an-unused-branch","title":"Delete an unused branch:","text":"<ul> <li><code>git branch --delete|-d BRANCHNAME</code> : it deletes the local branch BRANCHNAME</li> <li><code>git push -d origin BRANCHNAME</code> : it deletes the remote branch BRANCHNAME</li> </ul>"},{"location":"dev/git/#move-to-an-existing-local-branch","title":"Move to an existing local branch:","text":"<ul> <li><code>git checkout DESTINATIONBRANCH</code>: it moves the tracking to the local branch DESTINATIONBRANCH</li> </ul>"},{"location":"dev/git/#merge-a-branch","title":"Merge a branch","text":"<p>Sample procedure:</p> <ul> <li><code>git checkout -b NEW-FEATURE main</code> : Start a new feature by creating the branch NEW-FEATURE from MAIN</li> <li><code>git add &lt;file&gt;</code>: add a new file in the feature</li> <li><code>git commit -m \"Start a feature\"</code>: commit the edit</li> <li><code>git checkout main</code>: move to main</li> <li><code>git merge NEW-FEATURE</code>: merge the branch into main</li> <li><code>git branch -d NEW-FEATURE</code>: delete the branch</li> <li><code>git push</code>: sync with remote repo</li> </ul> <p>Warning</p> <p>If you get the <code>not something we can merge</code> error, it's probably because you don't have a local copy of the branch that you want to merge, as explained  here. Go on with: <pre><code>git checkout BRANCH-NAME\ngit checkout main\ngit merge BRANCH-NAME\n</code></pre></p>"},{"location":"dev/git/#switch-remote-urls-from-https-to-ssh","title":"Switch remote URLs from HTTPS to SSH","text":"<p>From your local project folder, <pre><code>$ git remote -v\n&gt; origin  https://github.com/USERNAME/REPOSITORY.git (fetch)\n&gt; origin  https://github.com/USERNAME/REPOSITORY.git (push)\n</code></pre> Set <pre><code>git remote set-url origin git@github.com:USERNAME/REPOSITORY.git\n</code></pre> Then verify that the remote URL has changed: <pre><code>$ git remote -v\n# Verify new remote URL\n&gt; origin  git@github.com:USERNAME/REPOSITORY.git (fetch)\n&gt; origin  git@github.com:USERNAME/REPOSITORY.git (push)\n</code></pre></p>"},{"location":"dev/git/#gitignore","title":".gitignore","text":"<ul> <li> <p>Compile your <code>\\.gitignore</code> file to prevent git from tracking files. Sample file: <pre><code>*.exe                       # Ignore _all_ exe files\n.virtualenvironment/        # Ignore the folder .virtualenvironment and its whole content\n</code></pre></p> </li> <li> <p>Make git forget about a file that's been tracked since now:</p> <ul> <li><code>git rm --cached &lt;file&gt;</code> - for the single file</li> <li><code>git rm -r --cached &lt;folder&gt;</code> - for a whole folder and all files in it recursively The removal of the file from the head revision will happen on the next commit.  Warning: While this will not remove the physical file from your local, it will remove the files from other developers machines on next git pull</li> </ul> </li> </ul>"},{"location":"dev/git/#misc","title":"Misc.","text":"<ul> <li> <p>Additional stuff: <code>git gui</code>: it opens the integrated git GUI <code>gitk</code>: it opens the commit viewer di Git</p> </li> <li> <p>Just reinstall git: <pre><code>sudo apt-get purge git\nsudo apt-get autoremove\nsudo apt-get install git\n</code></pre></p> </li> </ul>"},{"location":"dev/github/","title":"github","text":""},{"location":"dev/github/#getting-started","title":"Getting started","text":""},{"location":"dev/github/#setup-ssh-connection-with-github","title":"Setup SSH connection with GitHub","text":"<p>When you set up SSH, you will need to generate a new SSH key and add it to the ssh-agent. You must add the SSH key to your account on GitHub before you use the key to authenticate.</p>"},{"location":"dev/github/#check-for-existing-keys","title":"Check for existing keys","text":"<p>First of all, check if there is any existing key supported by GitHub: <pre><code>ls -al ~/.ssh | grep -E 'id_rsa.pub|id_ecdsa.pub|id_ed25519.pub' </code></pre></p>"},{"location":"dev/github/#create-a-new-key","title":"Create a new key","text":"<p>If no supported key is available, create a SSH key with: <pre><code>ssh-keygen -t ed25519 -C \"your_email@example.com\"\n</code></pre> It will prompt you for a secure passphrase. Then start the SSH agent: <pre><code>eval \"$(ssh-agent -s)\"\n</code></pre> and add your SSH private key to your SSH agent (replace <code>id_ed25519</code> if different). This ensure you won't have to reenter your passphrase every time you use your SSH key. <pre><code>ssh-add ~/.ssh/id_ed25519\n</code></pre> Add your newly created SSH key to your GitHub account from the browser, by copying the whole content of <code>cat ~/.ssh/id_ed25519.pub</code> (replace <code>ed25519</code> if different) in Your GitHub profile &gt; Settings &gt; SSH and GPG keys &gt; New SSH key, giving it a Title and confirming with Add SSH key</p>"},{"location":"dev/github/#test-your-ssh-connection","title":"Test your SSH connection","text":"<p>Test your connection with <code>ssh -T git@github.com</code>. A successful response should be like: <pre><code>Hi USERNAME! You've successfully authenticated, but GitHub does not provide shell access.\n</code></pre></p> <p>Danger</p> <p>If it doesn't work, troubleshoot the  Error: permission denied (publickey) error.</p>"},{"location":"dev/github/#github-actions","title":"Github Actions","text":""},{"location":"dev/github/#set-up-a-self-hosted-runner","title":"Set up a self-hosted runner","text":""},{"location":"dev/github/#add-a-self-hosted-runner-on-your-repository","title":"Add a self-hosted runner on your repository","text":"<ul> <li>Go on Your GitHub profile &gt; Your repo &gt; Settings &gt; Actions &gt; Runners &gt; New self-hosted runner</li> <li>Choose your OS (macOS, Linux, Windows) and architecture</li> <li><code>cd</code> in the path where you will store the runner</li> <li>Download the runner with <pre><code># Create a folder\n$ mkdir actions-runner &amp;&amp; cd actions-runner\n# Download the latest runner package\n$ curl -o actions-runner-linux-x64-2.284.0.tar.gz -L https://github.com/actions/runner/releases/download/v2.284.0/actions-runner-linux-x64-2.284.0.tar.gz\n# Optional: Validate the hash\n$ echo \"1ddfd7bbd3f2b8f5684a7d88d6ecb6de3cb2281a2a359543a018cc6e177067fc  actions-runner-linux-x64-2.284.0.tar.gz\" | shasum -a 256 -c\n# Extract the installer\n$ tar xzf ./actions-runner-linux-x64-2.284.0.tar.gz\n</code></pre></li> <li>Configure the runner with: <pre><code># Create the runner and start the configuration experience\n$ ./config.sh --url https://github.com/&lt;YOUR_USERNAME&gt;/&lt;YOUR_REPOSITORY&gt; --token &lt;YOUR_TOKEN&gt;\n# Last step, run it!\n$ ./run.sh\n</code></pre> When the runner is actually running, you will see it with the :green-circle: Idle label in Settings &gt; Actions &gt; Runner.</li> </ul>"},{"location":"dev/github/#configure-your-runner-as-a-service","title":"Configure your runner as a service","text":"<p>In the runner folder (<code>actions-runner</code>), </p> <ul> <li>Stop the runner if it is running</li> <li>Install the service with <pre><code>sudo ./svc.sh install\n</code></pre></li> <li>Start the service with <pre><code>sudo ./svc.sh start\n</code></pre></li> <li>Check the service status with <pre><code>sudo ./svc.sh status\n</code></pre></li> </ul>"},{"location":"dev/github/#use-the-self-hosted-runner-in-a-workflow","title":"Use the self-hosted runner in a workflow","text":"<p>To use the runner, add in your <code>.github/workflows/WORFLOW_NAME.yml</code> file: <pre><code># Use this YAML in your workflow file for each job\nruns-on: self-hosted\n</code></pre></p>"},{"location":"dev/github/#define-a-workflow","title":"Define a workflow","text":""},{"location":"dev/github/#generic-env-variables","title":"Generic env variables","text":"<ul> <li><code>${GITHUB_WORKSPACE}</code>: the project root folder (i.e. the one containing the <code>.github\\workflows</code> subfolder)</li> <li><code>${{ github.event.repository.name }}</code>: the project name, in our case <code>the-notebook</code></li> <li><code>${{ github.repository_owner }}</code>: the repository owner, in our case <code>sannae</code> (i.e. myself!)</li> </ul>"},{"location":"dev/github/#use-the-secrets-as-env-variables","title":"Use the secrets as env variables","text":"<p>Once you've defined a GitHub Secret, you can access it in your workflow as an environment variable from the step/job/workflow scope:</p> <pre><code>      - name: Your step name\n\n# Define an environment variable ACCESS_TOKEN \n# whose value is read from the GitHub Secret with the same name\nenv:\nACCESS_TOKEN: \"${{ secrets.ACCESS_TOKEN }}\"\nrun: python3 ${GITHUB_WORKSPACE}/scripts/lang-tracker.py\n</code></pre> <p>In the example above, the script <code>lang-tracker.py</code> is able to see the environment variable with the <code>os.environ[]</code> function:</p> <pre><code>g = Github(os.environ['ACCESS_TOKEN'])\n</code></pre>"},{"location":"dev/github/#github-api","title":"Github API","text":""},{"location":"dev/github/#test-the-api-via-console-using-curl","title":"Test the API via console (using <code>curl</code>)","text":""},{"location":"dev/github/#details-of-the-user","title":"Details of the user:","text":"<p><pre><code>user=$\"YOUR-GITHUB-USERNAME\"\ntoken=$\"YOUR-GITHUB-PERSONAL-ACCESS-TOKEN\"\ncurl -i -u \"$user:$token\" https://api.github.com/users/$user`\n</code></pre> The switch <code>-i</code> will display also the HTTP headers: notice the <code>content-type</code> header (it should be <code>application/json</code>) and the <code>x-ratelimit-limit</code> header (maximum amount of available request per hour, it should be 5000 for authenticated requests - usage is tracked by the <code>x-ratelimit-remaining</code>).</p>"},{"location":"dev/github/#details-of-the-users-repos-both-public-and-private","title":"Details of the user's repos (both public and private):","text":"<pre><code>user=$\"YOUR-GITHUB-USERNAME\"\ntoken=$\"YOUR-GITHUB-PERSONAL-ACCESS-TOKEN\"\ncurl -i -H \"Authorization: token $token\" https://api.github.com/users/$user/repos\n</code></pre>"},{"location":"dev/github/#details-of-a-specific-repo","title":"Details of a specific repo","text":"<pre><code>user=$\"YOUR-GITHUB-USERNAME\"\ntoken=$\"YOUR-GITHUB-PERSONAL-ACCESS-TOKEN\"\nrepo=$REPO_NAME\ncurl -i -H \"Authorization: token $token\" https://api.github.com/repos/$user/$repo\n</code></pre>"},{"location":"dev/github/#commits-of-a-specific-repo","title":"Commits of a specific repo","text":"<pre><code>user=$\"YOUR-GITHUB-USERNAME\"\ntoken=$\"YOUR-GITHUB-PERSONAL-ACCESS-TOKEN\"\nrepo=$REPO_NAME\ncurl -i -H \"Authorization: token $token\" https://api.github.com/repos/$user/$repo/commits\n</code></pre>"},{"location":"dev/github/#language-statistics-of-a-specific-repo","title":"Language statistics of a specific repo","text":"<pre><code>user=$\"YOUR-GITHUB-USERNAME\"\ntoken=$\"YOUR-GITHUB-PERSONAL-ACCESS-TOKEN\"\nrepo=$REPO_NAME\ncurl -i -H \"Authorization: token $token\" https://api.github.com/repos/$user/$repo/languages\n</code></pre>"},{"location":"dev/github/#calling-the-api-with-pygithub","title":"Calling the API with  PyGithub","text":"<p>Simply install with <pre><code>pip install PyGithub\n</code></pre></p> <p>Then use it by calling all the supported Github objects: <pre><code>from github import Github\n\ng = Github(\"YOUR_ACCESS_TOKEN_HERE\")\n\nfor repo in g.get_user().get_repos():\n  print(repo_name)\n</code></pre></p>"},{"location":"dev/gitpod/","title":"gitpod","text":"<ul> <li>Getting started tutorial</li> </ul>"},{"location":"hw/raspberrypi3/","title":"Raspberry Pi 3","text":"<p>Resources</p> <ul> <li>\"Getting started\" documentation: https://www.raspberrypi.com/documentation/computers/getting-started.html </li> </ul>"},{"location":"hw/raspberrypi3/#initial","title":"Initial","text":"<ul> <li>Update all the packages <pre><code>sudo apt-get update &amp;&amp; sudo apt-get upgrade\n</code></pre></li> <li>Set up DNS<ul> <li>Install ddclient: <pre><code>sudo apt-get install ddclient\n</code></pre></li> <li>Configure ddclient by editing <code>/etc/ddclient.conf</code> <pre><code>use=web \nssl=yes \nprotocol=PROVIDER\nlogin=MY_USERNAME\npassword='MY_PASSWORD'\nFULL_HOSTNAME\n</code></pre></li> <li>Restart the service with <code>sudo service ddclient restart</code></li> <li>Remember to create <code>FULL_HOSTNAME</code> on <code>PROVIDER</code></li> </ul> </li> <li>Set up static IP address <pre><code># Set a static ip by editing /etc/dhcpcd.conf\ninterface [eth0,wlan0] \nstatic ip_address=STATIC_IP_ADDRESS\nstatic routers=ROUTER_IP_ADDRESS\nstatic domain_name_servers=DNS_IP_ADDRESS\n</code></pre></li> <li>Set port forwarding on router</li> <li>Open SSH connection<ul> <li>Edit the file <code>/etc/ssh/sshd_config</code>: <pre><code>port=PORT_NUMBER  # Change SSH default port \nPermitRootlogin=no entry # Changed SSH login of root \nAllowUsers=user # Allowed SSH login of 'user'\nBanner=PATH_OF_BANNER_FILE # Created ssh-banner file containing a warning, then edited Banner entry with the path\n</code></pre></li> <li>Restart the service with <code>/etc/init.d/ssh restart</code></li> <li>Basically, now you can login as 'user' and then switch to a different administrator user</li> </ul> </li> <li>Set up dev environment</li> <li>VS code (<code>apt-get install code</code>)</li> <li>Git (<code>apt-get instaLL git</code>)</li> <li>Ruby (<code>apt-get install ruby-full</code>) an Rails (<code>gem install rails</code>)</li> </ul>"},{"location":"hw/raspberrypi3/#set-up-an-external-storage","title":"Set up an external storage","text":"<p>Full instructions here.</p> <ul> <li>Install exFAT driver: it may be useful in mounting the HDD storage <pre><code>apt-get install exfat-fuse\n</code></pre></li> <li>listing all the disk partitions and indentifying the one corresponding to the USB driver) <pre><code>lsblk -o UUID,NAME,FSTYPE,SIZE,MOUNTPOINT,LABEL,MODEL </code></pre></li> <li>Getting the location of the disk partition, usually <code>/dev/DISK_LOCATION</code> <pre><code>blkid\n</code></pre></li> <li>Creating a target folder to be the mount point of the USB drive <pre><code>mkdir /mnt/MOUNT_POINT_NAME </code></pre></li> <li>*Mounting the drive from the location of the partition to the mount point <pre><code>mount /dev/DISK_LOCATION /mnt/MOUNT_POINT_NAME\n</code></pre></li> <li>Check the content of the drive <pre><code>ls -l /mnt/MOUNT_POINT_NAME\n</code></pre></li> <li>Setting automounting<ul> <li>Retrieving the UUID <pre><code>blkid\n</code></pre></li> <li>editing the file <code>/etc/fstab</code> with the location of all partitions to be mounted at boot. Add the line <pre><code>UUID=&lt;UUID&gt; /mnt/MOUNT_POINT_NAME FORMAT BOOT_OPTIONS 0 2\n</code></pre></li> </ul> </li> <li>Format the USB drive in VFAT<ul> <li>retrieving the partition location <pre><code>lsblk\n</code></pre></li> <li>unmounting: it's mandatory before formatting <pre><code>umount /dev/PARTITION\n</code></pre></li> <li>formatting with VFAT <pre><code>mkfs.vfat -n 'PARTITION_NAME' /dev/PARTITION  </code></pre> USB key -made of one single VFAT partition- perfectly usable!</li> </ul> </li> </ul>"},{"location":"hw/raspberrypi3/#install-jupyter-notebook","title":"Install Jupyter Notebook","text":"<ul> <li>Install python3 <pre><code>apt-get update\napt-get install python3-matplotlib\napt-get install python3-scipy\npip3 install --upgrade pip\n</code></pre></li> <li>Reboot</li> <li>Install jupyter <pre><code>sudo pip3 install jupyter\nsudo apt-get clean\n</code></pre></li> <li>SSH-ing with a specific user ID runs the Jupyter Notebook from <code>/run/user/ID</code>, which may not have the permissions.  This comes from the env variable <code>$XDG_RUNTIME_DIR</code>, which does not change after substituting user. Just go with: <pre><code>export XDG_RUNTIME_DIR=\"\"\n</code></pre> (but since it's uncomfortable to set it at each login, just edit <code>~/.profile</code> adding <code>export XDG_RUNTIME_DIR</code>)</li> <li>Run jupyter notebook: <pre><code>jupyter notebook\n</code></pre></li> </ul> <p>Warning</p> <p>If port 8888 is still not reachable from SSH, go to PuTTY <code>Settings &gt; Tunnels &gt; Source port = 8888, Destination=127.0.0.1:8888</code></p> <ul> <li>Now your Jupyter Notebook is working from browser on &lt;127.0.0.1:8888/TOKEN&gt; !</li> </ul>"},{"location":"hw/raspberrypi3/#next-steps","title":"Next steps","text":"<ul> <li>Making an SD gzipped image of the system and uploading it in the cloud (e.g. Dropbox)</li> <li>Mounting an external HDD + Installing OwnCloud/NextCloud</li> <li>Continuing on SSH security: DSA public key authentication</li> </ul>"},{"location":"langs/bash/","title":"bash","text":""},{"location":"langs/bash/#basic","title":"Basic","text":"<ul> <li><code>ln -s TARGET LINK_NAME</code> ( man page): creates the soft link LINK_NAME of the file FILE</li> </ul>"},{"location":"langs/csharp/","title":"csharp","text":""},{"location":"langs/csharp/#basic","title":"Basic","text":""},{"location":"langs/csharp/#advanced","title":"Advanced","text":""},{"location":"langs/csharp/#task-async-await-event-handlers","title":"<code>Task</code>, <code>async</code>, <code>await</code>, Event handlers","text":"<p>:material-warning: TBD</p> <ul> <li>A synchronous job using <code>Thread.Sleep()</code> to wait for 5 seconds before completion (execution time: ~5030ms):</li> </ul> <pre><code>using System;\nusing System.Collections.Generic;\nusing System.IO;\nusing System.Threading.Tasks;\nusing System.Runtime.CompilerServices;\n\nnamespace Kitchen\n{\nclass Program\n{\nstatic async Task Main(string[] args)\n{\nProgram program = new Program();\nprogram.MakeFoodSync(\"Lentils soup\");\n}\n\npublic void MakeFoodSync(string foodItem)\n{\n// start timer\nvar watch = System.Diagnostics.Stopwatch.StartNew();\n\nConsole.WriteLine($\"Preparing {foodItem}...\");\nThread.Sleep(5000); // &lt;---- Notice the Thread namespace\nConsole.WriteLine($\"{foodItem} is ready!\");\n\n// stop timer\nwatch.Stop();\nConsole.WriteLine(\"Elapsed : \" + watch.ElapsedMilliseconds);\n}\n}\n}\n</code></pre> <ul> <li>An asynchronous job using <code>await Task.Delay()</code> (execution time: ~5030ms)</li> </ul> <pre><code>using System;\nusing System.Collections.Generic;\nusing System.IO;\nusing System.Threading.Tasks;\nusing System.Runtime.CompilerServices;\n\nnamespace Kitchen\n{\nclass Program\n{\nstatic async Task Main(string[] args)\n{\nProgram program = new Program();\nawait program.MakeFoodAsync(\"Lentils soup\");\n}\n\npublic async Task MakeFoodAsync(string foodItem)\n{\n// start timer\nvar watch = System.Diagnostics.Stopwatch.StartNew();\n\nConsole.WriteLine($\"Preparing {foodItem}...\");\nawait Task.Delay(5000);\nConsole.WriteLine($\"{foodItem} is ready!\");\n\n// stop timer\nwatch.Stop();\nConsole.WriteLine(\"Elapsed : \" + watch.ElapsedMilliseconds);\n}\n}\n}\n</code></pre>"},{"location":"langs/csharp/#yield-return","title":"<code>yield return</code>","text":"<p>:material-warning: TBD</p> <p>https://exercism.org/tracks/csharp/exercises/accumulate</p> <pre><code>using System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.IO;\n\npublic class Program\n{\n\n// Initial array\nvar array = new[] { \"A\", \"B\", \"C\", \"D\"};\n\npublic static void Main()\n{\n// Calling Test()\n}\n\n// Interface for enumerable objects\n// Used by: Lists, Arrays, Dictionaries, etc.\npublic IEnumerable&lt;string&gt; Test() {\n\n// Con ToList l'oggetto array viene processato _tutto insieme_\n// Quindi viene allocata memoria per l'oggetto nella sua interezza:\n// return array.ToList();\n\n// Altra maniera \"manuale\" di allocare tutto l'oggetto enumerabile\n// Usando una lista 'temp' da riempire man mano\nvar temp = New List&lt;string&gt;();\nforeach (var item in array){\ntemp.Add(item);\n}\nreturn temp;\n\n// In questa modalit\u00e0, invece, il return non chiude la funzione\n// Bens\u00ec all'oggetto chiamante ritorna man mano il risultato\n// dell'iterazione\nforeach (var item in array)\n{\nyield return item;\n}\n\n// Esempio classico: lettura di un file di grandi dimensioni\n// Qui sotto alloco il contenuto del file nella sua interezza\nvar file = File.OpenRead(\"filePath.txt\");\n// Qui invece leggo ed elaboro una riga alla volta\nusing (var sr = new StreamReader(File.OpenRead(\"filePath.txt\")))\n{\nwhile (!sr.EndOfStream){\n\nvar line = sr.ReadLine();\n\n// Se si volesse fermare l'iterazione in corrispondenza\n// di una determinata condizione (es. riga vuota):\n// if (line.IsEmpty)\n//      yield break;\n\nyield return line;\n}\n}\n}\n\n// Vari modi per consumare un enumerable\npublic void TestConsumeEnumerable() {\nvar enumerable = Test();\n\n// ToArray: un modo di consumare un enumerable\nvar values = enumerable.ToArray();\n\n// Enumerator: altro modo di consumare un enumerable\nvar enumerator = enumerable.GetEnumerator();\nwhile (enumerator.MoveNext())\n{\nvar item = enumerator.Current;\n}\n}\n\n}\n</code></pre>"},{"location":"langs/csharp/#iasyncenumerable","title":"<code>IAsyncEnumerable</code>","text":"<p>Same as before with <code>IAsyncEnumerable</code>:</p> <pre><code>using System;\nusing System.Collections.Generic;\nusing System.Linq;\n\npublic class Program\n{\nvar array = new[] {\"A\", \"B\", \"C\", \"D\"};\n\npublic static void Main()\n{\n// Calling Test()\n}\n\npublic async Task TestConsumeEnumerable(){\nvar enumerable = Test();\nawait foreach (var item in enumerable)\n{\nConsole.WriteLine(item);\n// Save on database\n// ...\n}\n}\n\npublic async IAsyncEnumerable&lt;string&gt; Test()\n{\nusing (var sr = new StreamReader(File.OpenRead(\"filePath.txt\")))\n{\nwhile (!sr.EndOfStream){\nvar line = await sr.ReadLineAsync();\nyield return line;\n}\n}\n}\n}\n</code></pre>"},{"location":"langs/csharp/#nested-types","title":"Nested Types","text":"<p>:material-warning: TBD</p> <p> cannot reference a type through an expression  calling the nested class from the parent class</p> <pre><code>// Outer class\npublic class OuterClass\n{\n// Property\npublic string OuterClassProperty { get; set; }\nprivate Speed currentSpeed;\n\n// inner class\npublic class InnerClass {\n\n// Instance of the parent class passed to the inner class' constructor\n// So that you can call outer's methods from inside inner\nprivate OuterClass _parentCar;\npublic InnerClass(OuterClass parentCar){\n_parentCar = parentCar;\n}\n\npublic void ShowSponsor(string sponsorName) {\n_parentCar.SetSponsor(sponsorName);\n}\n\npublic void Calibrate(){\n}\n\npublic bool SelfTest() =&gt; true;\n\npublic void SetSpeed(decimal amount, string unitsString){\nSpeedUnits speedUnits = SpeedUnits.MetersPerSecond;\nif (unitsString == \"cps\")\nspeedUnits = SpeedUnits.CentimetersPerSecond;\n_parentCar.SetSpeed(new Speed(amount, speedUnits));\n}\n\n}\n\n// Properties\npublic InnerClass Telemetry { get; set; }\npublic Speed CarSpeed { get; set; }\n\n// Constructor with nested classes\npublic OuterClass() {\nTelemetry = new InnerClass(this);\nCarSpeed = new Speed(0, SpeedUnits.MetersPerSecond);\n}\n\npublic string GetSpeed() =&gt; currentSpeed.ToString();\n\nprivate void SetSponsor(string sponsorName)\n{\nOuterClassProperty = sponsorName;\n}\n\nprivate void SetSpeed(Speed speed)\n{\ncurrentSpeed = speed;\n}\n}\n</code></pre>"},{"location":"langs/csharp/#operator-overloading","title":"Operator overloading","text":"<p>:material-warning: TBD</p> <p>https://exercism.org/tracks/csharp/exercises/hyperia-forex</p>"},{"location":"langs/csharp/#reflection-methods","title":"Reflection methods","text":""},{"location":"langs/csharp/#get-the-list-of-properties-of-an-object-in-order-of-declaration","title":"Get the list of properties of an object in order of declaration","text":"<pre><code>    // https://stackoverflow.com/questions/9062235/get-properties-in-order-of-declaration-using-reflection\nvar properties = typeof(YOUR_TYPE_HERE)\n.GetProperties(BindingFlags.Instance | BindingFlags.Public | BindingFlags.NonPublic)\n.OrderBy(x =&gt; x.MetadataToken);\n\n// just printing\nforeach(var property in properties){ Console.WriteLine(property.Name); }\n</code></pre>"},{"location":"langs/javascript/","title":"javascript","text":"<p>Resources</p> <ul> <li> Javascript for Beginners by Microsoft Developers</li> </ul>"},{"location":"langs/javascript/#intro","title":"Intro","text":"<ul> <li>Client: it runs in a web browser using <code>&lt;script&gt;</code> tags; it has access to the web page functions and objects (i.e. the Document Object Model or DOM)</li> <li>Server: the  Node.js runtime executes JavaScript on the server, it has access to built-in and third-party packages; usually used for building web services.</li> <li>Native: build native desktop and mobile applications with Electron, React Native and NativeScript</li> </ul>"},{"location":"langs/javascript/#nodejs","title":"Nodejs","text":"<p>Node.js is a JavaScript runtime to execute scripts out of the browser.</p> <p>To install Node.js, use  nvm (Node Version Manager) following the  install instructions on the official GitHub repository. Test it with just <code>nvm</code>.</p> <p>Then install Node.js: <pre><code>nvm install node\n</code></pre> And test it with <pre><code>node -v\n</code></pre> To run any JavaScript application (as simple as <code>console.log('Ciao mondo! \ud83e\udd59');</code>), use <pre><code>node APPLICATION_NAME.js\n</code></pre></p>"},{"location":"langs/javascript/#notes","title":"Notes","text":""},{"location":"langs/javascript/#variables","title":"Variables","text":"<p><pre><code>const greeting = \"Ciao\"\nconst place = \"Mondo\ud83c\udf0d\"\nconsole.log(\"%s, %s!\", greeting, place)\n</code></pre> <pre><code>const greeting = \"Ciao\"\nconst place = \"Mondo\ud83c\udf0d\"\nconsole.log(`${greeting}, ${place}!`)\n</code></pre></p>"},{"location":"langs/pwsh/","title":"powershell","text":"<p>Resources</p> <ul> <li> Powershell Masterclass by John Savill</li> </ul>"},{"location":"langs/python/","title":"Python","text":"<p>Resources</p> <ul> <li>Setting up Python 3.x in VS Code</li> </ul>"},{"location":"langs/python/#basics","title":"Basics","text":""},{"location":"langs/python/#package-management","title":"Package management","text":"<p>About package management</p> <ul> <li><code>py -m pip install --upgrade pip</code> : it installs and upgrades pip</li> <li><code>py -m pip install MODULE</code>: it installs the required module</li> </ul>"},{"location":"langs/python/#creating-and-using-virtual-environments","title":"Creating and using virtual environments","text":"<p>About virtual environments</p> <ul> <li><code>py -m venv .VIRTUALENVIRONMENTNAME</code>: it creates/uses the virtual environment specified in the path, then selectable in the terminal</li> <li><code>py -m deactivate</code>: it destroys the current virtual environment</li> </ul>"},{"location":"langs/python/#requirements","title":"Requirements","text":"<ul> <li><code>py -m pip freeze &gt; requirements.txt</code>: it prints a file called requirements with all the package necessary to the current project (they can be retrieved by the command <code>py -m pip install -r requirements.txt</code>)</li> </ul>"},{"location":"langs/python/#inheritance","title":"Inheritance","text":"<p>About inheritance</p>"},{"location":"langs/python/#testing","title":"Testing","text":""},{"location":"langs/python/#using-selenium-and-chromedriver","title":"Using Selenium and ChromeDriver","text":"<ul> <li> Download and install Chrome </li> </ul> <pre><code># Setup key\nwget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -\n\n# Setup repository\nsudo sh -c 'echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" &gt;&gt; /etc/apt/sources.list.d/google.list'\nsudo apt-get update \nsudo apt-get install google-chrome-stable\n</code></pre> <ul> <li> <p>Install Selenium and  <code>webdriver_manager</code> (this last one will take care of installing and updating the right version on the chromedriver, regardless of the local version of Google Chrome) <pre><code>pip install selenium\npip install webdriver_manager\n</code></pre></p> </li> <li> <p>In your Python script, add <pre><code>from selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.chrome.options import Options\n</code></pre></p> </li> <li> <p>To silence the webdriver, use <pre><code># Shut webdriver manager logs up\nos.environ['WDM_LOG_LEVEL'] = '0'\n</code></pre></p> </li> <li> <p>To set up a browser session: <pre><code>chrome_options = Options()\nchrome_options.add_argument(\"--headless\") # Means with no interface\nbrowser = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n</code></pre> To close the session after the tests: <pre><code>browser.quit()\n</code></pre></p> </li> </ul>"},{"location":"web/aspnetcore/","title":"ASP.NET Core []","text":"<p>Resources</p> <ul> <li>To be completed</li> </ul> <p></p> <p></p>"},{"location":"web/aspnetcore/#creating-the-project-with-the-dotnet-cli","title":"Creating the project with the <code>dotnet</code> cli","text":"<p>Obviously you're going to need the latest .NET SDK (check it with <code>dotnet check</code>).</p>"},{"location":"web/aspnetcore/#standard-new-templates","title":"Standard <code>new</code> templates","text":"<pre><code>dotnet new mvc --name MyMvcWebApplicationProjectName\ndotnet new sln --name MyMvcWebApplicationSolutionName\ndotnet sln ./MyMvcWebApplicationSolutionName.sln add ./MyMvcWebApplicationProjectName.csproj \n</code></pre> <p>!!! Info: Using an OrchardCore Project</p> <pre><code>The following example is to create a new project with an OrchardCoreCMS template in the current directory:\n\n```powershell\ndotnet new install OrchardCore.ProjectTemplates # Optional: to install 'dotnet new' templates\ndotnet new occms --name OrchardCoreTest\ndotnet new sln --name OrchardCoreTest\ndotnet sln ./OrchardCoreTest.sln add ./OrchardCoreTest.csproj\ndotnet build --verbosity normal # Optional: testing that the project is building\ndotnet run # Run Kestrel\n```\n</code></pre>"},{"location":"web/aspnetcore/#using-entity-framework-core","title":"Using Entity Framework Core","text":"<p> Following the Entity Framework Core for Beginners series on the .NET YouTube channel!</p>"},{"location":"web/aspnetcore/#creating-a-database","title":"Creating a database","text":"<ul> <li>Install NuGet packages:<ul> <li><code>Microsoft.EntityFrameworkCore</code></li> <li><code>Microsoft.EntityFrameworkCore.Design</code></li> <li><code>Microsoft.EntityFrameworkCore.Tools</code></li> <li><code>Microsoft.EntityFrameworkCore.SqlServer</code> (if you're planning to use SqlServer)</li> </ul> </li> <li>In VSCode, use <pre><code>dotnet tool install --global dotnet-ef\n</code></pre></li> <li>Create the classes in <code>/Models</code> mirroring the data models</li> <li>Create the database context in <code>/Data/DATABASENAMEContext.cs</code><ul> <li>The connection string should not be hardcoded: instead it should be securely stored, e.g. follow here</li> <li>If you don't want/need to create it manually, you could scaffold it together with the corresponding Controller</li> </ul> </li> </ul> <p>One way to do this is by storing them in a separate JSON file using User Secrets ([YourProject]&gt;[Manage User Secrets]) that you can exclude from source control. </p> <p>User secrets are automatically added to the <code>appsettings.json</code> configuration file, so they're available in the <code>builder.Configuration</code> object. </p> <p>The snippet to be used when configuring the db context in <code>Program.cs</code> is <pre><code>builder.Services.AddDbContext&lt;assistenza_backupContext&gt;(options =&gt;\noptions.UseSqlServer(builder.Configuration.GetConnectionString(\"PARAMETERNAME\")));\n</code></pre> Where the parameter will appear in your <code>secrets.json</code> as: <pre><code>{\n\"ConnectionString:PARAMETERNAME\": \"Data Source=(localdb)\\\\SQLEXPRESS; ...\"\n}\n</code></pre> in VSCode, use <code>dotnet user-secrets init</code> then <code>dotnet user-secrets set \"PROPERTY\" \"VALUE\"</code></p> <p> <code>secrets.json</code> is not encrypted!</p> <p>If you don't wish to use the User Secrets, you can add the connection string in the configuration file, for example: <pre><code>  \"ConnectionStrings\": {\n\"DBNAME\": \"Server=SERVERNAME\\\\INSTANCENAME;Database=DBNAME;Trusted_Connection=True;\"\n}\n</code></pre></p> <ul> <li>Create the database with the first initial create migration<ul> <li>In VS, use the Package Manager Console and type <code>Add-migration InitialCreate</code></li> <li>In VSCode, <code>dotnet ef migrations add InitialCreate</code></li> </ul> </li> <li>Review the migration before committing it!</li> <li>Apply the migration using:<ul> <li>In VS, the command <code>Update-database</code> in the Package Manager Console</li> <li>In VSCode, <code>dotnet ef database update</code></li> </ul> </li> </ul>"},{"location":"web/aspnetcore/#using-an-existing-database","title":"Using an existing database","text":"<p>\u2b50 Start with this tutorial.</p> <ul> <li>From the Package Manager Console, use <code>Scaffold-DbContext \"CONNECTIONSTRING\" Microsoft.EntityFrameworkCore.SqlServer -ContextDir data -OutputDir Models -DataAnnotation</code> where:<ul> <li><code>\"CONNECTIONSTRING\"</code> is just the connection string, like <code>\"Server=(local)\\localdb;Database=DB_NAME;Trusted_Connection=True\"</code></li> <li><code>Microsoft.EntityFrameworkCore.SqlServer</code> is the provider</li> <li><code>-ContextDir</code> specifies the destination of the DbContext class</li> <li><code>-OutputDir</code> specifies the destination of the created Models</li> <li><code>-DataAnnotation</code> specifies the usage of Data Annotation</li> </ul> </li> <li>In VSCode, use <code>dotnet ef dbcontext scaffold ...</code> followed by the same options</li> <li> <p>Then, to match the scaffolded schema within the migration logic,</p> <ul> <li><code>Add-migration InitialCreate</code> to create the migration first script</li> <li>Manually remove the content of the <code>Up</code> method in the script</li> <li><code>Update-database</code> to apply the migration</li> </ul> </li> <li> <p>The fields in a model can be renamed by just changing the properties names (it will rename the corresponding db columns) and then migrating up</p> </li> </ul>"},{"location":"web/aspnetcore/#applying-migrations","title":"Applying migrations","text":"<ul> <li>\u274c When applying the migration (<code>upgrade-database...</code>) if you ever encounter the error <code>Execution Timeout Expired.  The timeout period elapsed prior to completion of the operation or the server is not responding. Could not create constraint or index. See previous errors. Operation cancelled by user. The statement has been terminated.</code> you may extend the command timeout using something like: <pre><code>builder.Services.AddDbContext&lt;YOUR_DB_CONTEXT&gt;(options =&gt;\noptions.UseSqlServer(builder.Configuration.GetConnectionString(\"YOUR_DB_CONTEXT_CONNECTION_STRING\"),\nopts =&gt; opts.CommandTimeout((int)TimeSpan.FromMinutes(5).TotalSeconds)));\n</code></pre></li> </ul>"},{"location":"web/aspnetcore/#connect-to-multiple-dbs","title":"Connect to multiple DBs","text":"<p>Note: https://khalidabuhakmeh.com/how-to-add-a-view-to-an-entity-framework-core-dbcontext</p>"},{"location":"web/aspnetcore/#first-try-connecting-to-multiple-providers-eg-ms-access-sql-server","title":"First try connecting to multiple providers (e.g. MS Access &amp; SQL Server )","text":"<p>Following for instance this DEV article.</p>"},{"location":"web/aspnetcore/#migrate-from-ms-access-to-sql-server","title":"Migrate from MS Access to SQL Server","text":"<ul> <li>Unfortunately, the MS Access JET provider for Entity Framework Core is not supported from EFCore&gt;=6.0.0 and compatibility would require a downgrade to &lt;5.0.0</li> <li>Therefore, the best way is to convert your MS Access database into a more recent and supported RDBMS, e.g. SQL Server<ul> <li>Download and open SQL Server Management Assistant (SSMA) for Access</li> <li>Convert all the objects in your MS Access database into object of a newly created empty SQL Server database</li> <li>Add the new ODBC link to the new SQL Server database</li> <li>In your UI/mask/report structure, update any linked table or query to the new data source</li> <li>\ud83c\udfad There you go!</li> </ul> </li> </ul>"},{"location":"web/aspnetcore/#finally-connecting-multiple-times-to-the-same-provider","title":"Finally, connecting multiple times to the same provider","text":"<p>Following for instance this CodeMaze article.</p> <ul> <li>Follow the instructions at the paragraph above for an existing database to scaffold the DbContext and the Models</li> <li>Add the new connection string wherever your database connection configuration is stored</li> </ul> <p>For instance, if you're using the user secrets or a separate json file, it may look like:</p> <p><code>json { \"ConnectionStrings\": {   \"FIRST_DBCONTEXT_NAME\": \"FIRST_DBCONTEXT_CONNECTION_STRING\",   \"SECOND_DBCONTEXT_NAME\": \"SECOND_DBCONTEXT_CONNECTION_STRING\"  } }</code></p> <ul> <li>Add the new context in <code>Program.cs</code> with something like: <pre><code>builder.Services.AddDbContext&lt;YOURNEWCONTEXTNAME&gt;(options =&gt;\noptions.UseSqlServer(builder.Configuration.GetConnectionString(\"CONNECTIONSTRINGNAME\")));\n</code></pre></li> <li>Add the first migration by specifying the context, with <code>Add-Migration MIGRATIONNAME -Context CONTEXTNAME</code></li> <li>Remove the content of the <code>Up</code> method in the first migration file</li> <li>Apply the first migration by using <code>Update-database -Context CONTEXTNAME</code></li> </ul>"},{"location":"web/aspnetcore/#scaffold-razor-pages","title":"Scaffold Razor Pages","text":"<p>(i.e. automatically create CRUD pages)</p> <ul> <li>Install <code>Microsoft.VisualStudio.Web.CodeGeneration.Design</code></li> <li> <p>For each object you wish to scaffold the Razor Pages for:</p> <ul> <li>In the Pages folder, add a new subfolder in your project, naming it like the object you want to scaffold (es. Interventions)</li> <li>Right-click on the new subfolder and Add a scaffolded item</li> <li>Select the Razor Pages with Entity Framework (CRUD) option</li> <li>Select the specified class you wish to scaffold</li> <li>Select the Database Context</li> <li>The scaffolding process creates new files for each object to handle the Index, Create, Edit, Delete an Details features, both in:<ul> <li><code>.cshtml</code> the actual C#-HTML page rendered</li> <li><code>.cshtml.cs</code> the code behind</li> </ul> </li> </ul> </li> <li> <p>In VSCode:</p> <ul> <li><code>dotnet add package Microsoft.VisualStudio.Web.CodeGeneration.Design</code>, </li> <li>then <code>dotnet tool install --global dotnet-aspnet-codegenerator</code></li> <li>then <code>dotnet aspnet-codegenerator razorpage --model MODELNAME --dataContext DBCONTEXTNAME --relativeFolderPath Pages/MODELNAMEs --referenceScriptLibraries</code></li> </ul> </li> <li> <p>Test the model directly on the web app by running the debugger on IISExpress and going to <code>http://localhost:PORT/MODELNAMEs</code></p> </li> </ul>"},{"location":"web/aspnetcore/#controllers","title":"Controllers","text":"<ul> <li> <p>Controllers are routed using the <code>app.MapControllerRoute</code> method of the <code>WebApplication</code> instance</p> <ul> <li>Multiple patterns can be used, the <code>default</code> one is <code>\"{controller=Home}/{action=Index}/{id?}</code><ul> <li>Meaning that the default route (<code>/</code>) is made by the <code>Home</code> controller (<code>HomeController</code>) on the default action <code>Index</code> with an optional <code>id</code></li> </ul> </li> </ul> </li> <li> <p>The default HTTP method on Controllers is <code>get</code> (therefore, it's like every controller has the <code>[HttpGet]</code> attribute in front of it, except when specified otherwise)</p> <ul> <li>For any POST method (like <code>Create</code> or <code>Edit</code>), both <code>[HttpGet]</code> and <code>[HttpPost]</code> methods are implemented, depending on the web request received<ul> <li>That's why there are two <code>Edit</code> methods (the first one shows the selected item's properties in the form, the second one updates the item with the edited values), but only one <code>Create</code> method </li> </ul> </li> </ul> </li> </ul>"},{"location":"web/aspnetcore/#a-little-bit-of-style","title":"A little bit of style","text":"<ul> <li>To play a bit with Bootstrap, you can use the free Bootswatch themes.<ul> <li>Example of CDN link for the Minty theme: https://www.bootstrapcdn.com/bootswatch/</li> <li>jQuery required: https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js</li> </ul> </li> </ul>"},{"location":"web/aspnetcore/#building-the-solution","title":"Building the solution","text":"<ul> <li>\u274c If you ever encounter the error <code>System.InvalidOperationException : No method 'public static IHostBuilder CreateHostBuilder(string[] args)' or 'public static IWebHostBuilder CreateWebHostBuilder(string[] args)' found on 'AutoGeneratedProgram'. Alternatively, WebApplicationFactory'1 can be extended and 'CreateHostBuilder' or 'CreateWebHostBuilder' can be overridden to provide your own instance.</code>, it looks like a Visual Studio bug. Just open your <code>Program.cs</code> file and launch build from there.</li> </ul>"},{"location":"web/dependency-injection/","title":"Dependency Injection","text":""},{"location":"web/dependency-injection/#introduzione","title":"Introduzione","text":""},{"location":"web/dependency-injection/#il-problema-delle-dipendenze","title":"Il problema delle dipendenze","text":"<p>Si consideri la seguente classe: <pre><code>public class Car\n{\nprivate readonly PetrolEngine _engine = new PetrolEngine();\npublic void StartEngine(){\n_engine.Start();\n}\n\n}\n\npublic class PetrolEngine{\npublic void Start(){\nConsole.WriteLine(\"Starting the Petrol Engine...\");\nMixPetrolAndAir();\nInjectMixture();\nIgniteBySpark();\n}\n}\n</code></pre> Come si pu\u00f2 vedere, l'oggetto <code>Car</code> dipende interamente nella sua implementazione da <code>PetrolEngine</code>: nel caso in cui si volesse cambiare tipo di motore (es. Diesel), bisognerebbe creare una nuova classe <code>DieselEngine</code>: <pre><code>public class DieselEngine{\npublic void Start(){\nConsole.WriteLine(\"Starting the Diesel Engine...\");\n// Do stuff to start the Diesel Engine\n}\n}\n</code></pre> E usare quella nell'oggetto <code>Car</code>, di fatto rendendomi la classe interamente dipendente dal tipo di motore che implementa (ovvero sarebbe una <code>DieselCar</code> o una <code>PetrolCar</code>, ma non una generica <code>Car</code>).</p> <p>In pi\u00f9, in assenza di un'instanziazione di <code>PetrolEngine</code>, l'oggetto <code>Car</code> non pu\u00f2 nemmeno essere instanziato e quindi non pu\u00f2 nemmeno essere testato per altre funzioni che non dipendano dall'esistenza di un motore.</p>"},{"location":"web/dependency-injection/#necessita-della-dependency-injection","title":"Necessit\u00e0 della Dependency Injection","text":"<p>Questo succede perch\u00e9 la classe <code>Car</code> non dovrebbe dipendere da implementazioni (<code>private readonly PetrolEngine _engine = new PetrolEngine();</code> oppure <code>private readonly DieselEngine _engine = new DieselEngine();</code>), ma da astrazioni - ovvero, interfacce.</p> <p>Aggiungiamo quindi l'interfaccia per il 'generico' motore: <pre><code>public interface ICarEngine\n{\nvoid Start();\n}\n</code></pre> Di cui i motori precedenti diventano implementazioni: <pre><code>public class PetrolEngine : ICarEngine\n{\npublic void Start()\n{\nConsole.WriteLine(\"Starting the Petrol Engine...\");\n// Do stuff to start the Petrol Engine\n}\n}\n\npublic class DieselEngine : ICarEngine\n{\npublic void Start()\n{\nConsole.WriteLine(\"Starting the Diesel Engine...\");\n// Do stuff to start the Diesel Engine\n}\n}\n</code></pre> Quindi l'implementazione hard-coded scritta sopra (<code>private readonly PetrolEngine _engine = new PetrolEngine();</code>) diventa l'implementazione di un'interfaccia, ovvero: <pre><code>public class Car\n{  // instantiate the interface  \nprivate readonly ICarEngine _carEngine;\n\n// constructor, including the engine\npublic Car(ICarEngine carEngine){\n_carEngine = carEngine;\n}\n\n// the rest ... ...\npublic void StartEngine(){\n_engine.Start();\n}  }\n</code></pre> In questa maniera, possiamo implementare separatamente la <code>PetrolCar</code> e la <code>DieselCar</code> o qualsiasi altra <code>*Car</code> semplicemente chiamando la classe astratta <code>Car</code> e passando il tipo di motore nel costruttore, avvantaggiandosi dell'interfaccia appena aggiunta: <pre><code>// Start car with petrol engine\nCar petrolCar = new Car(new PetrolEngine());\npetrolCar.StartEngine();\n\n// Start car with diesel engine\nCar dieselEngine = new Car(new DieselEngine());\npetrolCar.StartEngine();\n</code></pre> Potrei anche creare un tipo di motore finto, necessario giusto per testare altre parti della classe <code>Car</code> e risolvendo il problema di testabilit\u00e0 citato prima: <pre><code>// Virtual engine\npublic class TestEngine : ICarEngine\n{\npublic void Start(){\nConsole.WriteLine(\"Starting a test engine...\");\n};\n}\n</code></pre></p>"},{"location":"web/django/","title":"Django","text":"<p>The reference repository is  sannae/djangocrm.</p> <p>Resources</p> <ul> <li>The main Django documentation is available here.</li> <li>Another nice learning path is in  Microsoft Learn and in the  Microsoft Developers Youtube channel.</li> </ul> Tutorials <ul> <li> Create a CRM web app by Dennis Ivy</li> <li> Create an expense tracking web app</li> <li> Django tutorial</li> <li> Django Testing</li> <li> Django Testing Automation with GitHub Actions by VeryAcademy</li> <li> Django and Docker by VeryAcademy</li> </ul>"},{"location":"web/django/#requirements","title":"Requirements","text":"<ul> <li>Python (<code>python3 -V</code> on Linux, <code>python -V</code> on Windows)</li> <li>Django, check if it's installed with a simple <code>python -m django --version</code></li> </ul> <p>All the required Python packages are listed in <code>requirements.txt</code> (to be updatable with <code>pip freeze &gt; requirements.txt</code>), run <code>pip install -r requirements.txt</code> to load them in your environment.</p> <p> Always run <code>pip freeze</code> from a virtual environment! Or it will just go on filling with c**p when deploying from any Cloud platform.</p>"},{"location":"web/django/#random-notes","title":"Random notes","text":""},{"location":"web/django/#about-djangos-architecture-and-mvt-pattern","title":"About Django's architecture and MVT pattern","text":""},{"location":"web/django/#create-a-project","title":"Create a project","text":"<ul> <li>A Django project is \"A Python package \u2013 i.e. a directory of code \u2013 that contains all the settings for an instance of Django. This would include database configuration, Django-specific options and application-specific settings.\" (source and tutorial)</li> <li>To get started with a project:<ul> <li>Create a folder for your project: <code>mkdir PROJECT_NAME</code>:</li> <li>Move in your project folder: <code>cd PROJECT_NAME</code></li> <li>Remember to activate your virtual environment: <code>python3 -m venv venv</code> and <code>source venv/bin/activate</code></li> <li>Install the dependencies (<code>python3 -m pip install django</code>) and freeze them in a file (<code>python3 -m pip freeze &gt; requirements.txt</code>)</li> <li>Create the core of the project: <code>django startproject core .</code></li> <li>The command <code>startproject</code> will create the following folder structure in your project folder: <pre><code>manage.py           # django command-line utility (check \"python manage.py --help\")\ncore/           # main project folder\n    __init__.py         # empty file telling Python that this directory should be considered a package\n    settings.py         # all of your settings or configurations\n    urls.py             # URLs within the project\n    asgi.py             # entry point for your web servers if asgi server is deployed\n    wsgi.py             # entry point for your web servers if wsgi server is deployed\n</code></pre></li> </ul> </li> </ul>"},{"location":"web/django/#create-an-app","title":"Create an app","text":"<ul> <li>Within the project, there may be several apps: each app structure is created in your project folder with <code>py -m django startapp APPLICATION_NAME</code> (from the same directory as <code>manage.py</code>)<ul> <li>The app has the following structure: <pre><code>APPLICATION_NAME/           # main app folder\n    __init__.py         # empty file telling Python that this directory should be considered a package   \n    admin.py            # file used to register admin templates\n    apps.py             # list of apps\n    migrations/         # list of migrations\n        __init__.py         # empty file telling Python that this directory should be considered a package \n    models.py           # models of the app\n    tests.py            # tests included in the app, see the corresponding section\n    views.py            # views of the app\n</code></pre></li> <li>Add the <code>APPLICATION_NAME\\urls.py</code> to map the routes in your application, with the following default content to create the home path: <pre><code>from django.urls import path\nfrom . import views\n\nurlpatterns = [\npath('', views.index, name='index'),\n]\n</code></pre></li> <li>The path management must also be handled by the main <code>PROJECT_NAME\\urls.py</code> file, where you should add at the beginning: <pre><code>from django.urls import include, path\n</code></pre> and in the <code>urlpatterns</code> list (replacing <code>APPLICATION_NAME</code>): <pre><code>path('', include('APPLICATION_NAME.urls')),\n</code></pre></li> <li>Register the application in the <code>PROJECT_NAME\\apps.py</code> file and to add it to the <code>INSTALLED_APPS</code> list in <code>settings.py</code> or the project won't be able to load it when running!</li> <li>Add the following test content in <code>APPLICATION_NAME\\views.py</code>: <pre><code>from django.shortcuts import render\nfrom django.http import HttpResponse\n\ndef index(request):\n    return HttpResponse(\"Hello, world!\")\n</code></pre></li> </ul> </li> <li>The live web server is started with <code>py -m django manage runserver</code> and is reachable at http://localhost:8000</li> </ul>"},{"location":"web/django/#misc","title":"Misc.","text":"<ul> <li>Django follows the MVC architecture (Model-View-Controller), although it uses a non-idiomatic way of naming its parts: <pre><code>Idiomatic term | Django term | Meaning\nModel          | Model       | Contains all the business logic. At the very least the database access logicView           | Template    | Responsible for generating the HTML and other UI\nController     | View        | Contains the logic to tie the other parts together and to generate a response to a user request\n</code></pre> A schematic view is available below: </li> <li>Oversimplifying, to add a feature you   1) Update the model in <code>models.py</code> (if needed)   2) Create or update the corresponding view in <code>views.py</code>   3) If the new feature opens a new page, create the new html page in <code>templates/</code> and add it to <code>urls.py</code></li> <li>General application secrets (i.e. database user, database password, secret key, etc.) are decoupled from the application with a JSON file not tracked by Git and using the <code>get_secret</code> function in <code>settings.py</code>. The function is: <pre><code># Secrets\nwith open(os.path.join(BASE_DIR, 'secrets.json')) as secrets_file:\n    secrets = json.load(secrets_file)\n\ndef get_secret(setting, secrets=secrets):\n\"\"\"Get secret setting or fail with ImproperlyConfigured\"\"\"\n    try:\n        return secrets[setting]\n    except KeyError:\n        raise ImproperlyConfigured(\"Set the {} setting\".format(setting))\n</code></pre> Then create your git-untracked <code>secrets.json</code> file with the following structure: <pre><code>{\n\"SECRET_KEY\" : \"YOUR_SECRET_KEY\",\n\"DB_USER\": \"YOUR_DB_USER\",\n\"DB_PASSWORD\": \"YOUR_DB_PASSWORD\",\n...\n}\n</code></pre> So that you can call your secrets from within the rest of the app by using: <pre><code>SECRET_KEY = get_secret('SECRET_KEY')\n</code></pre></li> <li>In the admin site, the display shows the name of the <code>Customer</code> or the <code>Product</code> for the registered models. This information appears because we set the <code>__str__</code> method on our objects. The default display of any object is the value returned by <code>__str__</code>.</li> </ul>"},{"location":"web/django/#about-templates","title":"About templates","text":"<ul> <li>Django is embedded in HTML via template tags</li> <li>The views of the app call the templates saved in <code>APPLICATION_NAME/templates/APPLICATION_NAME</code> (according to a Django's convention)</li> <li> <p>The templates use a combination of HTML/CSS/JS and Django's <code>{% templatetags %}</code> syntax: this lets you modularize the code</p> <ul> <li>Template tags can also be used to create the usual blocks to be run inside the template:<ul> <li>if/else statements: <pre><code>{% if somethings %}\n    &lt;h3&gt;There are {{ somethings.length }} things&lt;/h3&gt;\n{% else %}\n    &lt;h3&gt;Nothing here!&lt;/h3&gt;\n{% endif %}\n</code></pre></li> <li>for loops: <pre><code>&lt;ul&gt;\n{% for thing in somethings %}\n    &lt;li&gt;{{ thing.name}}&lt;/li&gt;\n{% endfor %}\n&lt;/ul&gt;\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>The variables in the templates are called like in the corresponding views and rendered with the field <code>{{ variable }}</code></p> <ul> <li>They can even be piped to a specific function or filter within the double curly braces, like in <code>{{ variable.field | function }}</code></li> </ul> </li> <li>The HTML/CSS/JS templates use  Bootstrap</li> <li> Do not comment Django template tags with usual HTML comments, as described  in this Stack Overflow post!!  <pre><code>&lt;!-- this is the usual HTML comment --&gt;\n&lt;!-- {% This is an uncommented Django tag %} --&gt;\n&lt;!-- {#% This is a commented Django tag %#} --&gt;\n</code></pre></li> </ul>"},{"location":"web/django/#about-models","title":"About models","text":"<ul> <li> <p>In your <code>models.py</code> file, you can add a first test model like: <pre><code>class MODEL_NAME(models.Model):\n    pass\n</code></pre></p> </li> <li> <p>In using the <code>ForeignKey</code> relationship between a 'parent' field and a 'child' field in <code>models.py</code>, Django automatically adds a property to the parent to provide access to all children called <code>&lt;child&gt;_set</code>, where <code>&lt;child&gt;</code> is the name of the child object. Below an example: <pre><code>from django.db import models\nclass Product(models.Model):\n    name = models.TextField()\n    category = models.ForeignKey(\n        'Category', #The name of the model\n        on_delete=models.PROTECT\n    )\n\nclass Category(models.Model):\n    name = models.TextField()\n    # product_set will be automatically created\n</code></pre></p> </li> </ul>"},{"location":"web/django/#about-urls","title":"About URLs","text":"<p>This is how URLs work in Django:</p> <p></p>"},{"location":"web/django/#about-static-files","title":"About static files","text":"<ul> <li>To upload the static files into an AWS S3 bucket, check out the documentation of django-storages. You basically need the <code>django-storages</code> and <code>boto3</code> Python libraries, as well as the following additional settings in <code>settings.py</code>: <pre><code>AWS_ACCESS_KEY_ID = get_secret('AWS_ACCESS_KEY_ID')\nAWS_SECRET_ACCESS_KEY = get_secret('AWS_SECRET_ACCESS_KEY')\nAWS_STORAGE_BUCKET_NAME = get_secret('AWS_STORAGE_BUCKET_NAME')\nAWS_S3_HOST = 's3.eu-west-3.amazonaws.com'\nAWS_S3_REGION_NAME = 'eu-west-3'\nAWS_S3_FILE_OVERWRITE = False\nAWS_DEFAULT_ACL = None\nDEFAULT_FILE_STORAGE = 'storages.backends.s3boto3.S3Boto3Storage'\nSTATICFILES_STORAGE = 'storages.backends.s3boto3.S3Boto3Storage'\n</code></pre> Here's a nice tutorial on creating a Django project with static files on AWS S3 and Docker Compose. This one also is a very good article.</li> </ul>"},{"location":"web/django/#about-user-authentication","title":"About user authentication","text":"<ul> <li>Django comes with a built-in user management and authentication system, where you can manage: </li> <li>Simple users (<code>user</code>), unable to access the admin site</li> <li>Staff users (<code>staff</code>), i.e. accessing the admin site but unable to change Data</li> <li> <p>superusers: you can create one with <code>python manage.py createsuperuser</code></p> </li> <li> <p>To restrict the user's login, add the <code>@login_required(login_url='login')</code> decorator from <code>django.contrib.auth.decorators</code> above any restricted view in <code>views.py</code> [manual method]</p> </li> <li>Likewise, you don't want any logged-in user to be able to access the <code>'login'</code> or the <code>'register'</code> page: add the <code>if request.user.is_authenticated</code> in those views to handle it [ manual method ]</li> <li>Decorators can be listed in a dedicated <code>\\APPLICATION_NAME\\decorators.py</code> file. A decorator is a function that takes another function as a parameter. Decorators are called with the <code>@</code> symbol</li> <li>Adding a property to a user: check this documentation</li> </ul>"},{"location":"web/django/#sending-emails-eg-to-reset-the-users-password","title":"Sending emails (e.g. to reset the user's password)","text":"<ul> <li>The main settings are saved in the <code>settings.py</code> file under the <code>EMAIL_</code> parameters</li> <li>In our example, Gmail was used as the SMTP host; any external login attempt would be blocked by default by Gmail unless you allow \"less secure apps\" access (here's the link). BTW it doesn't work directly with MFA accounts, where you'd need a specific App password.</li> <li>The <code>urls.py</code> must met specific criteria: use the predefined Authentication Views from <code>django.contrib.auth</code> and remember to use the corresponding URLs' names</li> <li>If you want to customize all the pre-built forms used by Django's Authentication Views, you can find the templates' names within their definitions. Override the default in your <code>urls.py</code> by specifying <code>.as_view(template_name=\"accounts/TEMPLATE_NAME.html\")</code> in the URL line</li> </ul>"},{"location":"web/django/#about-database-and-relationships","title":"About database and relationships","text":"<ul> <li>To initiate the database, run <code>py -m manage migrate</code>: the database's settings are in <code>SETTINGS.py</code> and SQLite3 is the default.</li> <li>To run progressive migrations, edit your models then run <code>py -m manage makemigrations</code> to create your migration files (preparation files before actual migration) in <code>/APPLICATION_NAME/migrations/</code>. Remember to register your models in the admin panel to see them. <p>The <code>makemigrations</code> command uses the current list of migrations to get a starting point, and then uses the current state of your models to determine the delta (the changes that need to be made). It then generates the necessary code to update the database. </p> </li> <li>To view the SQL commands related to a specific migration, run <code>python manage.py sqlmigrate APPLICATION_NAME MIGRATION_NAME</code></li> <li>To list the migrations, <code>python manage.py showmigrations</code></li> <li>To retrieve data from the db, use this reference guide:   1) Open your Django shell (<code>py -m manage shell</code>)   2) Import all your models (<code>from APPLICATION_NAME.models import *</code>)   3) Specific tables are then available as objects with <code>TABLENAME.objects.all()</code> and other methods.       1) Example: to retrieve all the customers saved with the <code>Customer</code> method, run <code>Customer.objects.all()</code>      2) Example: to retrieve all the customers with a specific name, run <code>Customer.objects.all().filter(name=\"YOURNAME\")</code></li> <li>For foreign keys, you can use the <code>_set</code> property of the parent object: <pre><code>Product.customer_set.all()\n</code></pre></li> <li>Example of <code>INSERT</code> from <code>python manage.py shell</code>: <pre><code>first_customer = Customer(\n  name=\"Mary Ann\", \n  location=\"USA\",\n  ...)\nfirst_customer.save()\n</code></pre> or <pre><code>Product(name=\"Shoes\", customer=first_customer).save()\n</code></pre></li> <li>A function to specifically create random orders was implemented in the <code>management\\commands\\populate-db.py</code> function, inspired by this article.</li> </ul>"},{"location":"web/django/#postgresql","title":"Postgresql","text":"<ul> <li>After first testing, migrate the db from SQLite to PostgreSQL using these instructions.</li> </ul> <p>Warning</p> <p>When following the above link, if incurring in the error <code>ModuleNotFoundError: No module named 'django.db.migrations.migration'</code>, you may have deleted also the migrations in the <code>/django/db/migrations</code> folder. You may just need to  force reinstall Django.</p>"},{"location":"web/django/#tests","title":"Tests","text":"<ul> <li>Check out the Django documentation about testing tools!</li> <li>To get started with testing, create a folder <code>APPLICATION_NAME\\Tests</code> containing all your <code>test_WHATEVER.py</code> files, where WHATEVER includes models, views, forms, etc.<ul> <li>In this case you will have to delete the <code>APPLICATION_NAME\\tests.py</code> file, originally created with the app</li> <li>Remember to create the <code>__init__.py</code> file in the <code>Tests/</code> folder! Or the tests contained in it won't be found by the Django integrated test runner</li> </ul> </li> <li>To make Django go through the tests, just run <code>python3 -m manage test APPLICATION_NAME</code>: Django will recursively look for all the classes and functions whose name starts with <code>test*</code></li> <li>In any test, the <code>assert</code> statement is the one being tested: a failing test will return <code>AssertionError</code></li> <li>Assertions are part of the <code>SimpleTestCase</code> class, containing the most simple unit tests (like testing HTML responses, comparing URLs, verifying HTTP redirect, testing form fields, etc.)</li> </ul>"},{"location":"web/django/#deployment","title":"Deployment","text":"<ul> <li>Before deploying, remember to:<ul> <li>Turn <code>Debug = FALSE</code> in <code>settings.py</code></li> <li>Add the remote host to the <code>ALLOWED_HOSTS</code> in <code>settings.py</code>, like <pre><code>ALLOWED_HOSTS = [\n    get_secret('HEROKU_HOST'),      # For production purposes (Debug=FALSE)\n    \"127.0.0.1\"                     # For testing purposes (Debug=TRUE)\n    ]\n</code></pre></li> </ul> </li> </ul>"},{"location":"web/django/#deploy-on-heroku","title":"Deploy on Heroku","text":"<p>Your project needs the Gunicorn and Whitenoise pip modules installed</p> <ul> <li>After logging in (<code>heroku login -i</code>), connect to your Heroku app using the Heroku CLI an running <code>heroku git:remote --app=HEROKU_APP_NAME</code> to add a remote origin to your Git tracking in the project</li> <li>Add a <code>procfile</code> (no extension!) to your project: it's needed by Heroku to specify a process type. Inside of it, just type <code>web: gunicorn YOUR_APP_WSGI_NAME.wsgi --log-file -</code></li> <li>Remember to specific a build pack (i.e. Python) in your Heroku app settings</li> <li>In the manual deploy from the Heroku app page, you may need to remove some specific requirements' versions (as described in  this post) from <code>requirements.txt</code> (but first, remember to check this!)</li> <li>Heroku doesn't know how to serve static files, so it is better to install Whitenoise and use it in the <code>MIDDLEWARE</code> section of your <code>settings.py</code> file</li> </ul>"},{"location":"web/django/#deploy-on-docker","title":"Deploy on  Docker","text":"<ul> <li>Write your Dockerfile: the base image is the official Python Docker image as the Django image is deprecated. The application directory is copied in the workdir <code>/usr/src/app</code> and the requirements are installed using the <code>requirements.txt</code> file. Lastly, the <code>manage runserver</code> command is executed to start the web server. <pre><code># Base image\nFROM python\n\n# Working directory\nWORKDIR /usr/src/app\n\n# Install dependencies\nCOPY requirements.txt ./\nRUN pip install --upgrade pip\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy the whole app folder\nCOPY . .\n\n# Run web server\nCMD [ \"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\" ]\n</code></pre></li> <li>Build the image using <code>docker build -t my-django-image .</code> from the path of the Dockerfile (it may require a few minutes \ud83d\udd70)</li> <li>Start the container with <code>docker run --name my-django-cont -d -p 8000:8000 my-django-image</code></li> <li>Verify that your container has been created with <code>docker ps -a</code></li> <li>Open your web application on a browser with http://HOSTNAME:8000 where <code>HOSTNAME</code> is included in the <code>ALLOWED_HOSTS</code> list in <code>settings.py</code> </li> <li>[optional] Interact with your container with <code>docker exec -it my-django-cont bash</code></li> </ul> <p>Clean your environment</p> <ul> <li>docker stop my-django-cont</li> <li>docker rm my-django-cont</li> <li>docker image rm my-django-image </li> <li>docker image rm python</li> </ul>"},{"location":"web/django/#deploy-on-azure-web-apps","title":"Deploy on  Azure Web Apps","text":"<p> TBD</p>"},{"location":"web/django/#definitely-review","title":"Definitely review","text":"<ul> <li>Forms and Formsets</li> <li>Users' authentication and register page</li> <li>How to use decorators</li> </ul>"},{"location":"web/rails/","title":"Ruby on Rails","text":"<p>Resources</p> <ul> <li> Ruby on Rails for beginners</li> <li> Let's build with Ruby on Rails </li> </ul>"},{"location":"web/rails/#set-up-your-dev-environment","title":"Set up your dev environment","text":"<p>There are several good instllation guides online, such as GoRails and Seriva's WSL2 Rails setup.</p> <ul> <li>Update all packages: <code>sudo apt update &amp;&amp; sudo apt upgrade</code></li> <li>Install dependencies: <code>sudo apt install build-essential git procps curl</code></li> <li>Install brew: <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\necho 'eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"' &gt;&gt; /home/admin/.profile\neval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"\nbrew install gcc\n</code></pre></li> <li>Install rbenv: <code>brew install rbenv</code></li> <li>Install Ruby: <pre><code>sudo apt-get install -y zlib1g-dev\nrbenv install 3.0.2 --verbose\nrbenv global 3.0.2\nrbenv init # Copy in ~/.profile\n</code></pre></li> <li>Install Rails:<ul> <li>Install the yarn package manager: <code>npm install -g yarn</code></li> <li>Install node: <code>brew install node</code></li> <li>Install libsqlite3-dev: <code>apt-get install libsqlite3-dev</code></li> <li>Install the rails gem: <code>gem install rails</code></li> <li>Install webpacker: <code>rails webpacker:install</code></li> </ul> </li> </ul>"},{"location":"web/rails/#get-started","title":"Get started","text":"<ul> <li>To create an app (it automatically includes <code>git init</code> for the project): <code>rails new MY_APP_NAME</code></li> <li>To run the web server:  <pre><code>cd MY_APP_NAME\nrails server\n</code></pre></li> <li>Configure git: <pre><code>git config --global color.ui true\ngit config --global user.name \"YOUR-NAME\"\ngit config --global user.email \"YOUR-EMAIL\"\n</code></pre></li> </ul> <p>Create a Rails dockerized dev environment, guide here</p>"},{"location":"web/rails/#random-notes","title":"Random Notes","text":"<ul> <li>Ruby on Rails follows the Model-Controller-View (MVC) architectural pattern: models are database tables, controllers are actions performed on the models, views are HTML pages rendered on the browser</li> <li>To create a controller, <code>rails generate controller NAME [action]</code></li> <li>Views templates have <code>.html.erb</code> extensions</li> <li>Django's corresponding URLs are called routes (<code>\\config\\routes.rb</code>)</li> <li>The <code>root</code> route can be referenced to a specific action of a specific controller (like <code>root CONTROLLERNAME#ACTION</code>)</li> <li>To check the routes in this Rails project, run <code>rails routes</code> or go to http://127.0.0.1:3000/rails/info/routes</li> <li>A <code>resource</code> is generally anything that you want to reach with a URI and perform CRUD operations on. Simply put, it's a database table which is represented by a model, and acted on through a controller.</li> </ul>"},{"location":"web/reactjs/","title":"reactjs","text":"<p>Resources</p> <ul> <li> React docs</li> </ul> Tutorials <ul> <li> Build a stock app with ReactJs</li> </ul>"},{"location":"web/reactjs/#prerequisites","title":"Prerequisites","text":"<p>Install the latest version of <code>npm</code>  (or to upgrade your current version): <pre><code>npm install -g npm\n</code></pre> or simply <pre><code>apt-get install npm\n</code></pre> Enter the npm script environment with <code>npx</code>.</p>"},{"location":"web/reactjs/#get-started","title":"Get started","text":"<p>To create your first React app, use the <code>create-react-app</code> utility: <pre><code>npx create-react-app MY-APP\n</code></pre> It will take some time!</p> <p>The <code>create-react-app</code> will provide all the essential packages to start a React app, including: * <code>src/</code>: the actual code to be edited * <code>node_modules/</code>: the React packages, imported in each <code>js</code> file using the first row <code>import React from 'react'</code> * <code>public/</code>: it contains the actual website, i.e. the <code>index.html</code> * a convenient <code>readme.md</code> with all the instructions for using <code>create-react-app</code></p>"},{"location":"web/reactjs/#run-the-dev-server","title":"Run the dev server","text":"<p>To run the development server, <pre><code>cd MY-APP\nnpm start\n</code></pre> The browser should automatically refresh at each change, as soon as you save your files. If it doesn't, you probably have to  increase the frequency of the <code>inotify</code> event monitoring process.</p>"},{"location":"web/reactjs/#architecture","title":"Architecture","text":"<p>Reactjs is used to build Single-Page Apps (SPAs), meaning that each page and each component in the page is rendered as an independent javascript file. You may therefore implement each page or each component separately.</p> <p>The main html page is <code>/public/index.html</code>:</p> <pre><code>&lt;!doctype html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;\n    &lt;link rel=\"shortcut icon\" href=\"%PUBLIC_URL%/favicon.ico\"&gt;\n    &lt;title&gt;Hotline front-end&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div id=\"root\"&gt;&lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>The <code>root</code> element is rendered by <code>/src/index.js</code>: <pre><code>ReactDOM.render(\n&lt;App /&gt;,\ndocument.getElementById('root')\n);\n</code></pre> Which contains the main <code>App</code> element, the one where all the routing and component rendering is implemented: <pre><code>function App() {\nreturn (\n/* Your app here */\n);\n}\nexport default App;\n</code></pre></p>"},{"location":"web/reactjs/#data","title":"Data","text":"<p>Use mockaroo to create a large mock dataset in JSON format to feed your React App. </p>"}]}